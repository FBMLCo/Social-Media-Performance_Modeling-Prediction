{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4.1: Modeling Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the methodology\n",
    "> * Finalize bining of Target Variable\n",
    "* Create Train and Test datasets\n",
    "* Create a Random Forest pipeline\n",
    "* Define key parameters\n",
    "* Run the model on sub-train data set and test accuracy on the validation data set\n",
    "* Select 2 mots accurate models based on the hyper-parameters, run it to get the confusion matrix\n",
    "* Select Best RF model candidate and apply it to the main train/test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict as OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "import scikitplot as skplt\n",
    "\n",
    "# t_NSE dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# Activate Seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the file for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the file and creating a dataframe\n",
    "master_modeling = pd.read_csv(\n",
    "    \"C:/Users/fbaff/EPFL ML Python/5. Captsone/master_modeling.csv\",\n",
    "    low_memory=False,\n",
    "    skipinitialspace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 351)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the Unnamed column\n",
    "master_modeling.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "master_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the modeling phase (without text and not relevant features)\n",
    "df_modeling = master_modeling.drop([\"Title\", \"Post_ID\", \"Snippet\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 348)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of # of classes for the Target Variable 'All_Impact'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We will split the variable in 3 classes using Scikit Learn preprocessing function KBinDiscretizer with the following parameters: number of bins 3, encode: ordinal and strategy: quantile\n",
    "* Oridinal has been selected as we are trying to model a hierarchy between low and high tweet impact\n",
    "* Quantile implies an even number of data points per class which would shape the model to learn about features for each class equally (avoiding unbalance classes)\n",
    "* We may reconsider some of the value of the parameters depending on the modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_bin = master_modeling[[\"ALL_Impact\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process binizer\n",
    "est = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"quantile\")\n",
    "est.fit(ai_bin)\n",
    "new_ai = est.transform(ai_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., 30., 41., 80.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the edge of the different 3 bins\n",
    "est.bin_edges_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ai_df = pd.DataFrame(new_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ai_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling[\"All_impact bin\"] = new_ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    69658\n",
       "1.0    63049\n",
       "0.0    61777\n",
       "Name: All_impact bin, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling[\"All_impact bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original All_Impact feature\n",
    "df_modeling2 = df_modeling.drop(\n",
    "    [\"ALL_Impact\", \"TW_Hashtags\", \"ALL_Author\", \"TW_Account_Name\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new All Impact feature type into int64\n",
    "df_modeling2[\"All_impact bin\"] = df_modeling2[\"All_impact bin\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 345)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Company_Clarivate</th>\n",
       "      <th>Company_Informa</th>\n",
       "      <th>Company_Pearson</th>\n",
       "      <th>Company_RELX Group</th>\n",
       "      <th>Company_Thomson Reuters</th>\n",
       "      <th>Company_Wolters Kluwer</th>\n",
       "      <th>Country 2_Argentina</th>\n",
       "      <th>Country 2_Australia</th>\n",
       "      <th>Country 2_Belgium</th>\n",
       "      <th>Country 2_Brazil</th>\n",
       "      <th>Country 2_Canada</th>\n",
       "      <th>Country 2_Ecuador</th>\n",
       "      <th>Country 2_France</th>\n",
       "      <th>Country 2_Germany</th>\n",
       "      <th>Country 2_Hong Kong</th>\n",
       "      <th>Country 2_India</th>\n",
       "      <th>Country 2_Italy</th>\n",
       "      <th>Country 2_Japan</th>\n",
       "      <th>Country 2_Mexico</th>\n",
       "      <th>Country 2_Netherlands</th>\n",
       "      <th>Country 2_Other</th>\n",
       "      <th>Country 2_Philippines</th>\n",
       "      <th>Country 2_Russia</th>\n",
       "      <th>Country 2_Serbia</th>\n",
       "      <th>Country 2_Singapore</th>\n",
       "      <th>Country 2_South Africa</th>\n",
       "      <th>Country 2_Spain</th>\n",
       "      <th>Country 2_Switzerland</th>\n",
       "      <th>Country 2_United Arab Emirates</th>\n",
       "      <th>Country 2_United Kingdom</th>\n",
       "      <th>Country 2_United States</th>\n",
       "      <th>Country 2_Venezuela</th>\n",
       "      <th>ALL_Thread_Entry_Type_post</th>\n",
       "      <th>ALL_Thread_Entry_Type_reply</th>\n",
       "      <th>ALL_Thread_Entry_Type_share</th>\n",
       "      <th>TW_Account_Type_Not identified</th>\n",
       "      <th>TW_Account_Type_individual</th>\n",
       "      <th>TW_Account_Type_organisational</th>\n",
       "      <th>ALL_Impact</th>\n",
       "      <th>Log_TW_KredOutreach</th>\n",
       "      <th>Log_Nbreach</th>\n",
       "      <th>Log_TW_NbFollowers</th>\n",
       "      <th>Log_TW_NbFollowing</th>\n",
       "      <th>Log_TW_NbTweets</th>\n",
       "      <th>TW_Hashtags</th>\n",
       "      <th>ALL_Author</th>\n",
       "      <th>TW_Account_Name</th>\n",
       "      <th>All_impact bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043355</td>\n",
       "      <td>-0.026154</td>\n",
       "      <td>0.056330</td>\n",
       "      <td>0.096232</td>\n",
       "      <td>-0.079631</td>\n",
       "      <td>-0.052836</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>-0.210815</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>-0.051351</td>\n",
       "      <td>-0.202983</td>\n",
       "      <td>-0.097575</td>\n",
       "      <td>-0.069580</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.098674</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>-0.056834</td>\n",
       "      <td>-0.094849</td>\n",
       "      <td>-0.026164</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.283895</td>\n",
       "      <td>-0.005615</td>\n",
       "      <td>-0.053263</td>\n",
       "      <td>0.018941</td>\n",
       "      <td>-0.010457</td>\n",
       "      <td>0.144206</td>\n",
       "      <td>0.044637</td>\n",
       "      <td>-0.004079</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.031087</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>-0.137032</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.054138</td>\n",
       "      <td>-0.043732</td>\n",
       "      <td>0.082092</td>\n",
       "      <td>0.078893</td>\n",
       "      <td>-0.026632</td>\n",
       "      <td>-0.063639</td>\n",
       "      <td>-0.104980</td>\n",
       "      <td>0.150564</td>\n",
       "      <td>0.134542</td>\n",
       "      <td>0.175049</td>\n",
       "      <td>-0.071198</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-0.100016</td>\n",
       "      <td>-0.217041</td>\n",
       "      <td>0.096486</td>\n",
       "      <td>-0.092585</td>\n",
       "      <td>0.039103</td>\n",
       "      <td>0.064535</td>\n",
       "      <td>0.061666</td>\n",
       "      <td>0.087718</td>\n",
       "      <td>-0.012533</td>\n",
       "      <td>-0.117382</td>\n",
       "      <td>-0.018209</td>\n",
       "      <td>0.045420</td>\n",
       "      <td>-0.249552</td>\n",
       "      <td>0.033651</td>\n",
       "      <td>-0.044680</td>\n",
       "      <td>-0.068578</td>\n",
       "      <td>-0.040140</td>\n",
       "      <td>-0.078196</td>\n",
       "      <td>-0.103747</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.124268</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.139425</td>\n",
       "      <td>-0.037394</td>\n",
       "      <td>0.049886</td>\n",
       "      <td>-0.062134</td>\n",
       "      <td>-0.060832</td>\n",
       "      <td>-0.164551</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.057373</td>\n",
       "      <td>0.061015</td>\n",
       "      <td>-0.099528</td>\n",
       "      <td>0.092122</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>-0.060507</td>\n",
       "      <td>0.082275</td>\n",
       "      <td>-0.030721</td>\n",
       "      <td>-0.009196</td>\n",
       "      <td>0.103404</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>0.123454</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>-0.078763</td>\n",
       "      <td>-0.101461</td>\n",
       "      <td>-0.139242</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.044942</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>0.214478</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>-0.033285</td>\n",
       "      <td>0.013041</td>\n",
       "      <td>0.002767</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.073032</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>-0.208537</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.006714</td>\n",
       "      <td>-0.044881</td>\n",
       "      <td>-0.061890</td>\n",
       "      <td>0.051839</td>\n",
       "      <td>-0.026937</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>0.052348</td>\n",
       "      <td>-0.023600</td>\n",
       "      <td>-0.276123</td>\n",
       "      <td>0.043335</td>\n",
       "      <td>0.084157</td>\n",
       "      <td>0.105428</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>-0.206991</td>\n",
       "      <td>0.110575</td>\n",
       "      <td>0.118327</td>\n",
       "      <td>-0.052622</td>\n",
       "      <td>-0.067342</td>\n",
       "      <td>0.039185</td>\n",
       "      <td>0.058289</td>\n",
       "      <td>-0.039581</td>\n",
       "      <td>-0.196574</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>-0.109202</td>\n",
       "      <td>0.043915</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>-0.012197</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>-0.083293</td>\n",
       "      <td>-0.062876</td>\n",
       "      <td>-0.053426</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>-0.034261</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>-0.088582</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>0.116984</td>\n",
       "      <td>-0.101969</td>\n",
       "      <td>-0.003977</td>\n",
       "      <td>-0.069417</td>\n",
       "      <td>0.023163</td>\n",
       "      <td>-0.142741</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>-0.026530</td>\n",
       "      <td>0.065626</td>\n",
       "      <td>-0.038544</td>\n",
       "      <td>-0.201538</td>\n",
       "      <td>0.103404</td>\n",
       "      <td>-0.099365</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.020793</td>\n",
       "      <td>-0.140340</td>\n",
       "      <td>0.140372</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>-0.116414</td>\n",
       "      <td>0.194603</td>\n",
       "      <td>-0.135277</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>0.029427</td>\n",
       "      <td>-0.008776</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>-0.011882</td>\n",
       "      <td>0.162964</td>\n",
       "      <td>-0.080648</td>\n",
       "      <td>0.194173</td>\n",
       "      <td>-0.153809</td>\n",
       "      <td>-0.049357</td>\n",
       "      <td>-0.024295</td>\n",
       "      <td>-0.039917</td>\n",
       "      <td>-0.067261</td>\n",
       "      <td>0.164591</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>-0.085144</td>\n",
       "      <td>0.081889</td>\n",
       "      <td>0.068522</td>\n",
       "      <td>0.031911</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>-0.217855</td>\n",
       "      <td>-0.105306</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.091675</td>\n",
       "      <td>-0.076823</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>-0.052916</td>\n",
       "      <td>-0.074188</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>0.140869</td>\n",
       "      <td>-0.030874</td>\n",
       "      <td>-0.009588</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.022868</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>-0.088623</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>0.028239</td>\n",
       "      <td>0.061117</td>\n",
       "      <td>-0.135228</td>\n",
       "      <td>-0.064412</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>-0.021576</td>\n",
       "      <td>0.137166</td>\n",
       "      <td>-0.050954</td>\n",
       "      <td>-0.035294</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>-0.059123</td>\n",
       "      <td>0.061017</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.049927</td>\n",
       "      <td>-0.085276</td>\n",
       "      <td>-0.061554</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>0.044271</td>\n",
       "      <td>-0.014038</td>\n",
       "      <td>-0.086426</td>\n",
       "      <td>0.107076</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>-0.073354</td>\n",
       "      <td>-0.019613</td>\n",
       "      <td>0.024592</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>0.072306</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>0.056966</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>0.123942</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.041504</td>\n",
       "      <td>-0.034953</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.124756</td>\n",
       "      <td>-0.125610</td>\n",
       "      <td>-0.190776</td>\n",
       "      <td>-0.022278</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.049581</td>\n",
       "      <td>0.134206</td>\n",
       "      <td>0.148641</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.031158</td>\n",
       "      <td>0.124959</td>\n",
       "      <td>-0.066650</td>\n",
       "      <td>-0.042318</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.155924</td>\n",
       "      <td>-0.134644</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>0.106415</td>\n",
       "      <td>-0.092285</td>\n",
       "      <td>-0.039612</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.047201</td>\n",
       "      <td>-0.021126</td>\n",
       "      <td>-0.047668</td>\n",
       "      <td>0.055562</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>0.174601</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>-0.065643</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.088623</td>\n",
       "      <td>0.075439</td>\n",
       "      <td>-0.124858</td>\n",
       "      <td>0.071592</td>\n",
       "      <td>0.053894</td>\n",
       "      <td>0.044313</td>\n",
       "      <td>-0.137777</td>\n",
       "      <td>0.157715</td>\n",
       "      <td>0.072449</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.081746</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.093424</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>-0.137166</td>\n",
       "      <td>-0.109904</td>\n",
       "      <td>-0.055583</td>\n",
       "      <td>-0.098694</td>\n",
       "      <td>-0.025431</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.052089</td>\n",
       "      <td>4.043051</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>7.409136</td>\n",
       "      <td>2017btsfesta</td>\n",
       "      <td>titina_joner</td>\n",
       "      <td>tw account not identified</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062174</td>\n",
       "      <td>-0.016751</td>\n",
       "      <td>0.080119</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>-0.120394</td>\n",
       "      <td>-0.103285</td>\n",
       "      <td>-0.096608</td>\n",
       "      <td>-0.039890</td>\n",
       "      <td>0.037337</td>\n",
       "      <td>-0.032766</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.153741</td>\n",
       "      <td>-0.094130</td>\n",
       "      <td>0.022027</td>\n",
       "      <td>-0.071482</td>\n",
       "      <td>-0.011108</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>0.116313</td>\n",
       "      <td>-0.124186</td>\n",
       "      <td>-0.075311</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>-0.058485</td>\n",
       "      <td>0.194499</td>\n",
       "      <td>-0.055562</td>\n",
       "      <td>0.021705</td>\n",
       "      <td>-0.109795</td>\n",
       "      <td>-0.057237</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>-0.051704</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.016233</td>\n",
       "      <td>-0.065274</td>\n",
       "      <td>-0.036608</td>\n",
       "      <td>-0.034709</td>\n",
       "      <td>-0.091634</td>\n",
       "      <td>-0.148071</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.090956</td>\n",
       "      <td>0.097090</td>\n",
       "      <td>-0.032322</td>\n",
       "      <td>-0.100132</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.041680</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.021864</td>\n",
       "      <td>-0.086222</td>\n",
       "      <td>0.038954</td>\n",
       "      <td>-0.102376</td>\n",
       "      <td>-0.015225</td>\n",
       "      <td>-0.069756</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.105984</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>-0.024489</td>\n",
       "      <td>-0.117432</td>\n",
       "      <td>-0.140402</td>\n",
       "      <td>0.083211</td>\n",
       "      <td>-0.089830</td>\n",
       "      <td>-0.174052</td>\n",
       "      <td>0.065776</td>\n",
       "      <td>-0.007650</td>\n",
       "      <td>-0.035333</td>\n",
       "      <td>-0.020222</td>\n",
       "      <td>-0.063972</td>\n",
       "      <td>-0.043835</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>-0.044005</td>\n",
       "      <td>0.163601</td>\n",
       "      <td>0.068502</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>-0.079210</td>\n",
       "      <td>-0.042082</td>\n",
       "      <td>-0.054253</td>\n",
       "      <td>0.024417</td>\n",
       "      <td>0.028049</td>\n",
       "      <td>-0.008803</td>\n",
       "      <td>-0.091295</td>\n",
       "      <td>0.044915</td>\n",
       "      <td>0.014499</td>\n",
       "      <td>0.044440</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>-0.083442</td>\n",
       "      <td>0.126394</td>\n",
       "      <td>0.094445</td>\n",
       "      <td>0.018419</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>-0.071571</td>\n",
       "      <td>-0.076823</td>\n",
       "      <td>-0.079915</td>\n",
       "      <td>0.030599</td>\n",
       "      <td>0.074232</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.212023</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>-0.095744</td>\n",
       "      <td>0.061964</td>\n",
       "      <td>0.021362</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>-0.029961</td>\n",
       "      <td>-0.145989</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.200928</td>\n",
       "      <td>-0.016330</td>\n",
       "      <td>-0.053699</td>\n",
       "      <td>-0.075765</td>\n",
       "      <td>-0.044156</td>\n",
       "      <td>0.034451</td>\n",
       "      <td>-0.007209</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.083679</td>\n",
       "      <td>-0.012804</td>\n",
       "      <td>0.059699</td>\n",
       "      <td>0.046251</td>\n",
       "      <td>-0.054959</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>-0.216166</td>\n",
       "      <td>0.022366</td>\n",
       "      <td>0.051676</td>\n",
       "      <td>-0.096489</td>\n",
       "      <td>-0.057265</td>\n",
       "      <td>-0.034846</td>\n",
       "      <td>0.028727</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>-0.085795</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>-0.031386</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.056986</td>\n",
       "      <td>0.045627</td>\n",
       "      <td>0.051439</td>\n",
       "      <td>0.078362</td>\n",
       "      <td>0.110548</td>\n",
       "      <td>-0.084496</td>\n",
       "      <td>0.077610</td>\n",
       "      <td>0.091607</td>\n",
       "      <td>-0.032996</td>\n",
       "      <td>0.047377</td>\n",
       "      <td>-0.034858</td>\n",
       "      <td>-0.051805</td>\n",
       "      <td>-0.079793</td>\n",
       "      <td>-0.083378</td>\n",
       "      <td>0.150011</td>\n",
       "      <td>-0.073107</td>\n",
       "      <td>-0.121585</td>\n",
       "      <td>-0.012478</td>\n",
       "      <td>0.027656</td>\n",
       "      <td>-0.062822</td>\n",
       "      <td>-0.024272</td>\n",
       "      <td>-0.102905</td>\n",
       "      <td>-0.051975</td>\n",
       "      <td>-0.023164</td>\n",
       "      <td>-0.190809</td>\n",
       "      <td>0.129924</td>\n",
       "      <td>0.046678</td>\n",
       "      <td>-0.003092</td>\n",
       "      <td>-0.100179</td>\n",
       "      <td>-0.093669</td>\n",
       "      <td>0.057292</td>\n",
       "      <td>-0.047797</td>\n",
       "      <td>-0.021681</td>\n",
       "      <td>0.064168</td>\n",
       "      <td>-0.090251</td>\n",
       "      <td>0.151048</td>\n",
       "      <td>-0.054837</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>-0.058757</td>\n",
       "      <td>0.054199</td>\n",
       "      <td>0.029473</td>\n",
       "      <td>-0.028904</td>\n",
       "      <td>-0.003152</td>\n",
       "      <td>-0.108188</td>\n",
       "      <td>-0.082987</td>\n",
       "      <td>-0.154588</td>\n",
       "      <td>-0.089762</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>0.111226</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>-0.054118</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.085395</td>\n",
       "      <td>0.050117</td>\n",
       "      <td>0.028890</td>\n",
       "      <td>-0.066603</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>0.091539</td>\n",
       "      <td>0.080098</td>\n",
       "      <td>-0.097620</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>-0.094672</td>\n",
       "      <td>-0.032235</td>\n",
       "      <td>0.027303</td>\n",
       "      <td>0.088460</td>\n",
       "      <td>0.057590</td>\n",
       "      <td>-0.003974</td>\n",
       "      <td>0.018053</td>\n",
       "      <td>-0.060452</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>0.082648</td>\n",
       "      <td>-0.045213</td>\n",
       "      <td>-0.010520</td>\n",
       "      <td>0.071452</td>\n",
       "      <td>0.032315</td>\n",
       "      <td>-0.012261</td>\n",
       "      <td>-0.031019</td>\n",
       "      <td>-0.127360</td>\n",
       "      <td>0.060023</td>\n",
       "      <td>0.043121</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>-0.147651</td>\n",
       "      <td>-0.020718</td>\n",
       "      <td>-0.153849</td>\n",
       "      <td>0.042013</td>\n",
       "      <td>0.033189</td>\n",
       "      <td>0.031711</td>\n",
       "      <td>0.058556</td>\n",
       "      <td>-0.107727</td>\n",
       "      <td>0.099962</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>0.038591</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-0.030599</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.086928</td>\n",
       "      <td>-0.151598</td>\n",
       "      <td>0.137112</td>\n",
       "      <td>-0.047292</td>\n",
       "      <td>0.124308</td>\n",
       "      <td>-0.011943</td>\n",
       "      <td>0.043047</td>\n",
       "      <td>0.029912</td>\n",
       "      <td>-0.056634</td>\n",
       "      <td>-0.078559</td>\n",
       "      <td>0.008586</td>\n",
       "      <td>0.158098</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.105360</td>\n",
       "      <td>-0.048394</td>\n",
       "      <td>-0.185371</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.030467</td>\n",
       "      <td>-0.041006</td>\n",
       "      <td>0.192561</td>\n",
       "      <td>0.132670</td>\n",
       "      <td>-0.038194</td>\n",
       "      <td>0.107279</td>\n",
       "      <td>0.047092</td>\n",
       "      <td>-0.034917</td>\n",
       "      <td>-0.058675</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>0.149902</td>\n",
       "      <td>0.065286</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>-0.014648</td>\n",
       "      <td>-0.105659</td>\n",
       "      <td>-0.004008</td>\n",
       "      <td>0.031223</td>\n",
       "      <td>-0.032511</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.082954</td>\n",
       "      <td>0.102146</td>\n",
       "      <td>-0.021088</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.029705</td>\n",
       "      <td>-0.089681</td>\n",
       "      <td>-0.048299</td>\n",
       "      <td>-0.004015</td>\n",
       "      <td>0.097629</td>\n",
       "      <td>-0.077728</td>\n",
       "      <td>0.071031</td>\n",
       "      <td>-0.184252</td>\n",
       "      <td>0.097553</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>-0.013075</td>\n",
       "      <td>-0.156467</td>\n",
       "      <td>0.078722</td>\n",
       "      <td>0.075358</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.132711</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.031087</td>\n",
       "      <td>-0.015971</td>\n",
       "      <td>-0.003174</td>\n",
       "      <td>-0.017293</td>\n",
       "      <td>-0.091109</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>-0.040622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.940171</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>4.948760</td>\n",
       "      <td>8.307706</td>\n",
       "      <td>No TW hashtag</td>\n",
       "      <td>dasCameo1</td>\n",
       "      <td>tw account not identified</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005493</td>\n",
       "      <td>-0.046224</td>\n",
       "      <td>-0.041031</td>\n",
       "      <td>0.039266</td>\n",
       "      <td>-0.156698</td>\n",
       "      <td>0.102946</td>\n",
       "      <td>0.091634</td>\n",
       "      <td>-0.114136</td>\n",
       "      <td>0.035787</td>\n",
       "      <td>0.029237</td>\n",
       "      <td>-0.070190</td>\n",
       "      <td>-0.110189</td>\n",
       "      <td>-0.139811</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>-0.137965</td>\n",
       "      <td>0.136637</td>\n",
       "      <td>0.132650</td>\n",
       "      <td>0.043569</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>-0.111776</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.010579</td>\n",
       "      <td>0.226237</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>0.108195</td>\n",
       "      <td>-0.109070</td>\n",
       "      <td>-0.124105</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>-0.040324</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>0.047302</td>\n",
       "      <td>0.032003</td>\n",
       "      <td>-0.023468</td>\n",
       "      <td>-0.054362</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>-0.021037</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.036580</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>-0.031158</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>-0.060048</td>\n",
       "      <td>-0.051961</td>\n",
       "      <td>-0.077230</td>\n",
       "      <td>-0.097722</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>-0.066325</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.035116</td>\n",
       "      <td>0.034780</td>\n",
       "      <td>-0.036535</td>\n",
       "      <td>-0.018424</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>-0.004659</td>\n",
       "      <td>-0.108618</td>\n",
       "      <td>-0.092896</td>\n",
       "      <td>0.052488</td>\n",
       "      <td>-0.055745</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>-0.115885</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.083206</td>\n",
       "      <td>-0.038005</td>\n",
       "      <td>-0.043213</td>\n",
       "      <td>0.082052</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.028137</td>\n",
       "      <td>0.068420</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.125366</td>\n",
       "      <td>0.028890</td>\n",
       "      <td>-0.047038</td>\n",
       "      <td>0.097249</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>-0.035116</td>\n",
       "      <td>0.043783</td>\n",
       "      <td>0.017253</td>\n",
       "      <td>-0.033530</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.019368</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>-0.028239</td>\n",
       "      <td>0.037282</td>\n",
       "      <td>-0.048869</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>0.083659</td>\n",
       "      <td>0.107605</td>\n",
       "      <td>-0.021207</td>\n",
       "      <td>-0.061615</td>\n",
       "      <td>0.017141</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.029683</td>\n",
       "      <td>0.012526</td>\n",
       "      <td>0.042427</td>\n",
       "      <td>-0.126465</td>\n",
       "      <td>0.021342</td>\n",
       "      <td>0.041280</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>-0.093953</td>\n",
       "      <td>-0.065308</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.046895</td>\n",
       "      <td>0.025065</td>\n",
       "      <td>0.051971</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>-0.051707</td>\n",
       "      <td>0.129374</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>-0.017832</td>\n",
       "      <td>-0.104167</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>-0.082194</td>\n",
       "      <td>-0.022909</td>\n",
       "      <td>-0.034485</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.137858</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>-0.060527</td>\n",
       "      <td>-0.097707</td>\n",
       "      <td>0.058024</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.021011</td>\n",
       "      <td>-0.081706</td>\n",
       "      <td>0.082845</td>\n",
       "      <td>-0.010966</td>\n",
       "      <td>0.043193</td>\n",
       "      <td>0.052958</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>-0.082560</td>\n",
       "      <td>-0.038488</td>\n",
       "      <td>0.058573</td>\n",
       "      <td>-0.042765</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061991</td>\n",
       "      <td>-0.068532</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.024292</td>\n",
       "      <td>0.074849</td>\n",
       "      <td>0.031820</td>\n",
       "      <td>-0.092250</td>\n",
       "      <td>-0.020162</td>\n",
       "      <td>0.084524</td>\n",
       "      <td>-0.147868</td>\n",
       "      <td>-0.062541</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>-0.030426</td>\n",
       "      <td>-0.085693</td>\n",
       "      <td>0.051463</td>\n",
       "      <td>-0.010905</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.065308</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>-0.046076</td>\n",
       "      <td>-0.103394</td>\n",
       "      <td>0.090983</td>\n",
       "      <td>-0.017095</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>-0.115377</td>\n",
       "      <td>-0.037476</td>\n",
       "      <td>0.015483</td>\n",
       "      <td>0.017873</td>\n",
       "      <td>-0.157227</td>\n",
       "      <td>0.124105</td>\n",
       "      <td>-0.126567</td>\n",
       "      <td>-0.146362</td>\n",
       "      <td>-0.134094</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>-0.084229</td>\n",
       "      <td>0.075480</td>\n",
       "      <td>-0.031128</td>\n",
       "      <td>-0.122884</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>0.027629</td>\n",
       "      <td>0.117645</td>\n",
       "      <td>0.052917</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>-0.062907</td>\n",
       "      <td>-0.020345</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.075867</td>\n",
       "      <td>0.072021</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>-0.119954</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.131185</td>\n",
       "      <td>-0.037038</td>\n",
       "      <td>-0.102656</td>\n",
       "      <td>0.012527</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.048421</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.032928</td>\n",
       "      <td>-0.013875</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>-0.049927</td>\n",
       "      <td>-0.080343</td>\n",
       "      <td>-0.006144</td>\n",
       "      <td>-0.068746</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>-0.071228</td>\n",
       "      <td>-0.026810</td>\n",
       "      <td>0.022125</td>\n",
       "      <td>-0.042013</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.082031</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>-0.032837</td>\n",
       "      <td>0.036977</td>\n",
       "      <td>0.109833</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.034871</td>\n",
       "      <td>-0.023722</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>-0.007650</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>-0.093669</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.039879</td>\n",
       "      <td>-0.027364</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>-0.011678</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.019389</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>-0.001719</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.013428</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>-0.040639</td>\n",
       "      <td>-0.069377</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.117432</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.014323</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>-0.020671</td>\n",
       "      <td>-0.075439</td>\n",
       "      <td>0.097026</td>\n",
       "      <td>-0.049601</td>\n",
       "      <td>0.070302</td>\n",
       "      <td>0.055786</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.117442</td>\n",
       "      <td>-0.045333</td>\n",
       "      <td>-0.012736</td>\n",
       "      <td>0.039469</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>0.096029</td>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.075846</td>\n",
       "      <td>-0.077474</td>\n",
       "      <td>-0.045939</td>\n",
       "      <td>-0.027669</td>\n",
       "      <td>-0.007100</td>\n",
       "      <td>0.138590</td>\n",
       "      <td>-0.054138</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>-0.041967</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>-0.050761</td>\n",
       "      <td>-0.002299</td>\n",
       "      <td>-0.036357</td>\n",
       "      <td>-0.027110</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.099396</td>\n",
       "      <td>0.053792</td>\n",
       "      <td>0.046010</td>\n",
       "      <td>-0.034454</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>-0.030070</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>0.081278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.921578</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.286371</td>\n",
       "      <td>indeed, jobs</td>\n",
       "      <td>lumpyspace_tst2</td>\n",
       "      <td>tw account not identified</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034058</td>\n",
       "      <td>-0.047404</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>-0.056559</td>\n",
       "      <td>-0.054515</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>-0.080485</td>\n",
       "      <td>0.138346</td>\n",
       "      <td>-0.030263</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.129862</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>-0.125420</td>\n",
       "      <td>-0.010376</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>-0.061971</td>\n",
       "      <td>-0.094401</td>\n",
       "      <td>-0.003983</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>0.118225</td>\n",
       "      <td>0.113057</td>\n",
       "      <td>-0.011241</td>\n",
       "      <td>-0.026782</td>\n",
       "      <td>-0.072286</td>\n",
       "      <td>0.068011</td>\n",
       "      <td>-0.054321</td>\n",
       "      <td>-0.101929</td>\n",
       "      <td>-0.060425</td>\n",
       "      <td>0.097453</td>\n",
       "      <td>-0.058421</td>\n",
       "      <td>-0.060181</td>\n",
       "      <td>-0.028402</td>\n",
       "      <td>-0.018962</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>0.182556</td>\n",
       "      <td>0.014852</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.155111</td>\n",
       "      <td>0.067444</td>\n",
       "      <td>0.135010</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.141538</td>\n",
       "      <td>-0.074992</td>\n",
       "      <td>-0.040710</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>-0.041550</td>\n",
       "      <td>-0.003866</td>\n",
       "      <td>-0.134155</td>\n",
       "      <td>-0.006185</td>\n",
       "      <td>0.074931</td>\n",
       "      <td>0.120809</td>\n",
       "      <td>0.090902</td>\n",
       "      <td>-0.119643</td>\n",
       "      <td>0.101644</td>\n",
       "      <td>-0.278809</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.145426</td>\n",
       "      <td>-0.127767</td>\n",
       "      <td>0.014847</td>\n",
       "      <td>-0.141357</td>\n",
       "      <td>0.116096</td>\n",
       "      <td>-0.040685</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.038656</td>\n",
       "      <td>0.109090</td>\n",
       "      <td>0.056274</td>\n",
       "      <td>0.036662</td>\n",
       "      <td>-0.135010</td>\n",
       "      <td>0.173177</td>\n",
       "      <td>-0.001648</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>0.179932</td>\n",
       "      <td>0.028971</td>\n",
       "      <td>-0.080485</td>\n",
       "      <td>0.221110</td>\n",
       "      <td>-0.100530</td>\n",
       "      <td>-0.083669</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>0.065857</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>-0.232422</td>\n",
       "      <td>-0.104762</td>\n",
       "      <td>-0.052999</td>\n",
       "      <td>-0.110026</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>-0.028941</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>-0.050975</td>\n",
       "      <td>-0.037699</td>\n",
       "      <td>-0.076853</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>-0.080729</td>\n",
       "      <td>-0.055893</td>\n",
       "      <td>-0.058716</td>\n",
       "      <td>-0.089254</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>0.128062</td>\n",
       "      <td>-0.006266</td>\n",
       "      <td>-0.094218</td>\n",
       "      <td>-0.065043</td>\n",
       "      <td>-0.105754</td>\n",
       "      <td>0.087321</td>\n",
       "      <td>0.066366</td>\n",
       "      <td>0.071757</td>\n",
       "      <td>-0.018575</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.084330</td>\n",
       "      <td>0.060598</td>\n",
       "      <td>-0.108067</td>\n",
       "      <td>-0.050863</td>\n",
       "      <td>-0.081533</td>\n",
       "      <td>0.037760</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>-0.134115</td>\n",
       "      <td>-0.038045</td>\n",
       "      <td>0.126099</td>\n",
       "      <td>-0.011058</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>-0.182190</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>-0.063599</td>\n",
       "      <td>-0.038005</td>\n",
       "      <td>-0.198873</td>\n",
       "      <td>-0.048197</td>\n",
       "      <td>0.177531</td>\n",
       "      <td>0.087321</td>\n",
       "      <td>-0.027832</td>\n",
       "      <td>0.027425</td>\n",
       "      <td>0.062907</td>\n",
       "      <td>-0.031504</td>\n",
       "      <td>0.108826</td>\n",
       "      <td>-0.110575</td>\n",
       "      <td>-0.008031</td>\n",
       "      <td>-0.043432</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>-0.092855</td>\n",
       "      <td>0.064677</td>\n",
       "      <td>-0.066732</td>\n",
       "      <td>-0.019257</td>\n",
       "      <td>0.163859</td>\n",
       "      <td>-0.047719</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.065715</td>\n",
       "      <td>-0.017843</td>\n",
       "      <td>-0.023214</td>\n",
       "      <td>-0.034251</td>\n",
       "      <td>-0.024292</td>\n",
       "      <td>-0.148397</td>\n",
       "      <td>-0.087591</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>-0.027995</td>\n",
       "      <td>-0.118225</td>\n",
       "      <td>0.046631</td>\n",
       "      <td>-0.015864</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>0.055786</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.026100</td>\n",
       "      <td>0.104655</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>0.109594</td>\n",
       "      <td>-0.123479</td>\n",
       "      <td>-0.149984</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>-0.202881</td>\n",
       "      <td>-0.073079</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>-0.105367</td>\n",
       "      <td>0.041260</td>\n",
       "      <td>-0.039968</td>\n",
       "      <td>-0.138021</td>\n",
       "      <td>0.154226</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>-0.044647</td>\n",
       "      <td>-0.149801</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>-0.048991</td>\n",
       "      <td>-0.069743</td>\n",
       "      <td>-0.082250</td>\n",
       "      <td>0.074097</td>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>-0.090495</td>\n",
       "      <td>-0.193359</td>\n",
       "      <td>-0.033590</td>\n",
       "      <td>0.076864</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>-0.001863</td>\n",
       "      <td>0.023560</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.060735</td>\n",
       "      <td>0.156667</td>\n",
       "      <td>-0.030070</td>\n",
       "      <td>0.041056</td>\n",
       "      <td>0.186727</td>\n",
       "      <td>-0.068985</td>\n",
       "      <td>-0.060099</td>\n",
       "      <td>-0.156148</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.069865</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>-0.099202</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>-0.220357</td>\n",
       "      <td>0.093180</td>\n",
       "      <td>-0.028727</td>\n",
       "      <td>-0.059021</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>-0.141429</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>-0.033930</td>\n",
       "      <td>0.068970</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>-0.142741</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.099325</td>\n",
       "      <td>0.047994</td>\n",
       "      <td>-0.082382</td>\n",
       "      <td>-0.091553</td>\n",
       "      <td>-0.073730</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.075890</td>\n",
       "      <td>0.047760</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>-0.116648</td>\n",
       "      <td>-0.058116</td>\n",
       "      <td>-0.037130</td>\n",
       "      <td>-0.162435</td>\n",
       "      <td>-0.027201</td>\n",
       "      <td>0.134768</td>\n",
       "      <td>0.033610</td>\n",
       "      <td>-0.294027</td>\n",
       "      <td>0.055562</td>\n",
       "      <td>0.036784</td>\n",
       "      <td>-0.072001</td>\n",
       "      <td>0.126592</td>\n",
       "      <td>0.063838</td>\n",
       "      <td>-0.075948</td>\n",
       "      <td>-0.034203</td>\n",
       "      <td>-0.064128</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.131999</td>\n",
       "      <td>-0.014160</td>\n",
       "      <td>-0.016856</td>\n",
       "      <td>0.046305</td>\n",
       "      <td>0.146535</td>\n",
       "      <td>-0.031982</td>\n",
       "      <td>-0.058634</td>\n",
       "      <td>0.040497</td>\n",
       "      <td>-0.056701</td>\n",
       "      <td>-0.164917</td>\n",
       "      <td>-0.027507</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.167450</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>-0.087626</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>-0.041168</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-0.025584</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>0.109413</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>0.017263</td>\n",
       "      <td>0.059255</td>\n",
       "      <td>-0.066406</td>\n",
       "      <td>0.060918</td>\n",
       "      <td>-0.163411</td>\n",
       "      <td>0.153905</td>\n",
       "      <td>0.163147</td>\n",
       "      <td>0.088826</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.073853</td>\n",
       "      <td>-0.075114</td>\n",
       "      <td>0.035563</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.156169</td>\n",
       "      <td>-0.040385</td>\n",
       "      <td>-0.123149</td>\n",
       "      <td>-0.069814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.832808</td>\n",
       "      <td>6.736967</td>\n",
       "      <td>5.043425</td>\n",
       "      <td>4.564348</td>\n",
       "      <td>MiFIDII, TRRisk</td>\n",
       "      <td>mifidii</td>\n",
       "      <td>tw account not identified</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.072963</td>\n",
       "      <td>-0.063298</td>\n",
       "      <td>0.152832</td>\n",
       "      <td>0.010882</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.033273</td>\n",
       "      <td>0.140869</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>-0.156294</td>\n",
       "      <td>-0.061733</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.069789</td>\n",
       "      <td>-0.106934</td>\n",
       "      <td>-0.054827</td>\n",
       "      <td>0.025112</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.080845</td>\n",
       "      <td>0.037702</td>\n",
       "      <td>-0.060117</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>-0.004307</td>\n",
       "      <td>-0.040806</td>\n",
       "      <td>-0.151821</td>\n",
       "      <td>-0.083279</td>\n",
       "      <td>-0.164403</td>\n",
       "      <td>-0.061192</td>\n",
       "      <td>0.078081</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.080741</td>\n",
       "      <td>-0.009417</td>\n",
       "      <td>0.115635</td>\n",
       "      <td>0.035217</td>\n",
       "      <td>0.112374</td>\n",
       "      <td>0.042201</td>\n",
       "      <td>-0.042376</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.208060</td>\n",
       "      <td>0.014474</td>\n",
       "      <td>-0.224932</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.067732</td>\n",
       "      <td>-0.012835</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>-0.074463</td>\n",
       "      <td>-0.055219</td>\n",
       "      <td>-0.107753</td>\n",
       "      <td>-0.013218</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>-0.035505</td>\n",
       "      <td>-0.026005</td>\n",
       "      <td>-0.015050</td>\n",
       "      <td>-0.164568</td>\n",
       "      <td>-0.022893</td>\n",
       "      <td>-0.100307</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>-0.043648</td>\n",
       "      <td>-0.115108</td>\n",
       "      <td>0.088215</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>0.083182</td>\n",
       "      <td>-0.031023</td>\n",
       "      <td>0.036656</td>\n",
       "      <td>-0.252093</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>0.057425</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>-0.101493</td>\n",
       "      <td>0.013463</td>\n",
       "      <td>0.112584</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>0.080444</td>\n",
       "      <td>-0.059518</td>\n",
       "      <td>0.022269</td>\n",
       "      <td>-0.071045</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.101998</td>\n",
       "      <td>-0.048429</td>\n",
       "      <td>-0.178327</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.140695</td>\n",
       "      <td>-0.120571</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>-0.142273</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>-0.035359</td>\n",
       "      <td>-0.086966</td>\n",
       "      <td>0.190186</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.054199</td>\n",
       "      <td>-0.043039</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.042759</td>\n",
       "      <td>0.041024</td>\n",
       "      <td>-0.114572</td>\n",
       "      <td>-0.101214</td>\n",
       "      <td>-0.070565</td>\n",
       "      <td>0.161482</td>\n",
       "      <td>-0.078108</td>\n",
       "      <td>-0.028477</td>\n",
       "      <td>-0.098319</td>\n",
       "      <td>-0.035052</td>\n",
       "      <td>-0.018206</td>\n",
       "      <td>0.108965</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>-0.074986</td>\n",
       "      <td>0.216518</td>\n",
       "      <td>0.094064</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>0.030814</td>\n",
       "      <td>-0.114650</td>\n",
       "      <td>0.086496</td>\n",
       "      <td>0.052891</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>-0.088191</td>\n",
       "      <td>0.081020</td>\n",
       "      <td>-0.030252</td>\n",
       "      <td>0.023577</td>\n",
       "      <td>-0.175293</td>\n",
       "      <td>-0.041794</td>\n",
       "      <td>-0.133440</td>\n",
       "      <td>-0.080314</td>\n",
       "      <td>-0.114816</td>\n",
       "      <td>0.110439</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.002853</td>\n",
       "      <td>-0.061157</td>\n",
       "      <td>0.063041</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>-0.063198</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.088797</td>\n",
       "      <td>-0.110897</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>-0.124176</td>\n",
       "      <td>-0.042899</td>\n",
       "      <td>-0.087123</td>\n",
       "      <td>-0.062901</td>\n",
       "      <td>0.036656</td>\n",
       "      <td>-0.020194</td>\n",
       "      <td>0.068593</td>\n",
       "      <td>0.078413</td>\n",
       "      <td>-0.077096</td>\n",
       "      <td>-0.024309</td>\n",
       "      <td>-0.079450</td>\n",
       "      <td>-0.037545</td>\n",
       "      <td>-0.133440</td>\n",
       "      <td>-0.023856</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.053502</td>\n",
       "      <td>-0.112549</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>0.052420</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>0.071472</td>\n",
       "      <td>0.006378</td>\n",
       "      <td>-0.009972</td>\n",
       "      <td>0.055612</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.138939</td>\n",
       "      <td>-0.071307</td>\n",
       "      <td>-0.028948</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.279088</td>\n",
       "      <td>-0.097377</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>-0.005057</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>-0.153721</td>\n",
       "      <td>0.149937</td>\n",
       "      <td>-0.082419</td>\n",
       "      <td>0.012626</td>\n",
       "      <td>-0.013951</td>\n",
       "      <td>0.057329</td>\n",
       "      <td>-0.043588</td>\n",
       "      <td>-0.100307</td>\n",
       "      <td>-0.117083</td>\n",
       "      <td>0.140224</td>\n",
       "      <td>-0.054478</td>\n",
       "      <td>-0.083705</td>\n",
       "      <td>0.082014</td>\n",
       "      <td>-0.260812</td>\n",
       "      <td>-0.080963</td>\n",
       "      <td>0.151672</td>\n",
       "      <td>-0.012748</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>-0.022757</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>-0.067191</td>\n",
       "      <td>-0.104614</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>-0.001378</td>\n",
       "      <td>0.156948</td>\n",
       "      <td>-0.057055</td>\n",
       "      <td>-0.050502</td>\n",
       "      <td>-0.141741</td>\n",
       "      <td>0.073382</td>\n",
       "      <td>0.109767</td>\n",
       "      <td>0.082275</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.025940</td>\n",
       "      <td>-0.014805</td>\n",
       "      <td>0.055934</td>\n",
       "      <td>-0.062047</td>\n",
       "      <td>0.025705</td>\n",
       "      <td>-0.028050</td>\n",
       "      <td>-0.090768</td>\n",
       "      <td>0.048671</td>\n",
       "      <td>-0.003732</td>\n",
       "      <td>-0.025142</td>\n",
       "      <td>-0.002267</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.131958</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>-0.085118</td>\n",
       "      <td>0.112462</td>\n",
       "      <td>-0.132882</td>\n",
       "      <td>0.061558</td>\n",
       "      <td>-0.120219</td>\n",
       "      <td>0.115653</td>\n",
       "      <td>-0.071987</td>\n",
       "      <td>-0.088632</td>\n",
       "      <td>0.068566</td>\n",
       "      <td>-0.002302</td>\n",
       "      <td>-0.085798</td>\n",
       "      <td>-0.021868</td>\n",
       "      <td>-0.117746</td>\n",
       "      <td>-0.038783</td>\n",
       "      <td>0.058184</td>\n",
       "      <td>-0.040423</td>\n",
       "      <td>-0.178545</td>\n",
       "      <td>-0.037197</td>\n",
       "      <td>0.107509</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.093157</td>\n",
       "      <td>0.087437</td>\n",
       "      <td>-0.090663</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.106759</td>\n",
       "      <td>-0.033343</td>\n",
       "      <td>-0.038156</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>0.027335</td>\n",
       "      <td>0.049127</td>\n",
       "      <td>-0.015246</td>\n",
       "      <td>0.193080</td>\n",
       "      <td>-0.068952</td>\n",
       "      <td>-0.006553</td>\n",
       "      <td>-0.152274</td>\n",
       "      <td>-0.011509</td>\n",
       "      <td>-0.079381</td>\n",
       "      <td>0.107561</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>-0.004211</td>\n",
       "      <td>-0.046247</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>-0.010141</td>\n",
       "      <td>-0.023019</td>\n",
       "      <td>-0.053563</td>\n",
       "      <td>-0.034999</td>\n",
       "      <td>0.099792</td>\n",
       "      <td>-0.096793</td>\n",
       "      <td>-0.012102</td>\n",
       "      <td>-0.139858</td>\n",
       "      <td>0.112514</td>\n",
       "      <td>0.174438</td>\n",
       "      <td>0.099505</td>\n",
       "      <td>-0.081822</td>\n",
       "      <td>-0.012748</td>\n",
       "      <td>-0.051200</td>\n",
       "      <td>-0.071638</td>\n",
       "      <td>-0.083104</td>\n",
       "      <td>0.021170</td>\n",
       "      <td>0.153721</td>\n",
       "      <td>-0.019165</td>\n",
       "      <td>-0.055054</td>\n",
       "      <td>-0.116455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.408529</td>\n",
       "      <td>7.094235</td>\n",
       "      <td>7.222566</td>\n",
       "      <td>8.832150</td>\n",
       "      <td>infographic, iot</td>\n",
       "      <td>daviddefelipe</td>\n",
       "      <td>david de felipe</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.043355 -0.026154  0.056330  0.096232 -0.079631 -0.052836  0.048258   \n",
       "1  0.062174 -0.016751  0.080119  0.042372 -0.120394 -0.103285 -0.096608   \n",
       "2 -0.005493 -0.046224 -0.041031  0.039266 -0.156698  0.102946  0.091634   \n",
       "3  0.034058 -0.047404  0.014119 -0.056559 -0.054515 -0.006490 -0.003326   \n",
       "4  0.015451  0.072963 -0.063298  0.152832  0.010882 -0.013000  0.053467   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.210815  0.129344  0.028728 -0.051351 -0.202983 -0.097575 -0.069580   \n",
       "1 -0.039890  0.037337 -0.032766  0.028105 -0.153741 -0.094130  0.022027   \n",
       "2 -0.114136  0.035787  0.029237 -0.070190 -0.110189 -0.139811  0.046875   \n",
       "3 -0.080485  0.138346 -0.030263 -0.060120 -0.129862  0.015977  0.050049   \n",
       "4 -0.033273  0.140869  0.007952  0.001726 -0.156294 -0.061733  0.000837   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  0.023331  0.098674 -0.010539  0.010712 -0.056834 -0.094849 -0.026164   \n",
       "1 -0.071482 -0.011108 -0.013699  0.116313 -0.124186 -0.075311  0.020711   \n",
       "2 -0.137965  0.136637  0.132650  0.043569 -0.008219 -0.111776 -0.036621   \n",
       "3 -0.125420 -0.010376  0.087402 -0.001668 -0.061971 -0.094401 -0.003983   \n",
       "4 -0.119629  0.020996 -0.040771  0.069789 -0.106934 -0.054827  0.025112   \n",
       "\n",
       "         21        22        23        24        25        26        27  \\\n",
       "0 -0.002116  0.283895 -0.005615 -0.053263  0.018941 -0.010457  0.144206   \n",
       "1 -0.058485  0.194499 -0.055562  0.021705 -0.109795 -0.057237  0.024563   \n",
       "2 -0.010579  0.226237 -0.059814  0.108195 -0.109070 -0.124105  0.116130   \n",
       "3 -0.011739  0.118225  0.113057 -0.011241 -0.026782 -0.072286  0.068011   \n",
       "4  0.067383  0.080845  0.037702 -0.060117  0.173410 -0.004307 -0.040806   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0  0.044637 -0.004079  0.004364  0.031087 -0.100098 -0.137032 -0.058838   \n",
       "1  0.042552 -0.051704 -0.015205 -0.016233 -0.065274 -0.036608 -0.034709   \n",
       "2 -0.040324  0.021962  0.047302  0.032003 -0.023468 -0.054362 -0.019450   \n",
       "3 -0.054321 -0.101929 -0.060425  0.097453 -0.058421 -0.060181 -0.028402   \n",
       "4 -0.151821 -0.083279 -0.164403 -0.061192  0.078081  0.013227  0.080741   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0 -0.054138 -0.043732  0.082092  0.078893 -0.026632 -0.063639 -0.104980   \n",
       "1 -0.091634 -0.148071  0.003933 -0.090956  0.097090 -0.032322 -0.100132   \n",
       "2 -0.021037 -0.046631  0.149658 -0.017619  0.036580  0.000163 -0.017944   \n",
       "3 -0.018962  0.000183 -0.024658  0.182556  0.014852 -0.015625 -0.155111   \n",
       "4 -0.009417  0.115635  0.035217  0.112374  0.042201 -0.042376  0.069170   \n",
       "\n",
       "         42        43        44        45        46        47        48  \\\n",
       "0  0.150564  0.134542  0.175049 -0.071198 -0.067627 -0.100016 -0.217041   \n",
       "1  0.176303 -0.041680  0.032715 -0.021864 -0.086222  0.038954 -0.102376   \n",
       "2 -0.031158  0.001811  0.051025 -0.060048 -0.051961 -0.077230 -0.097722   \n",
       "3  0.067444  0.135010 -0.018148 -0.145508 -0.141538 -0.074992 -0.040710   \n",
       "4  0.004743  0.208060  0.014474 -0.224932 -0.008684 -0.067732 -0.012835   \n",
       "\n",
       "         49        50        51        52        53        54        55  \\\n",
       "0  0.096486 -0.092585  0.039103  0.064535  0.061666  0.087718 -0.012533   \n",
       "1 -0.015225 -0.069756  0.013082  0.105984  0.008850 -0.024489 -0.117432   \n",
       "2  0.016937 -0.066325 -0.012329  0.069517  0.035116  0.034780 -0.036535   \n",
       "3  0.129700 -0.041550 -0.003866 -0.134155 -0.006185  0.074931  0.120809   \n",
       "4  0.010725 -0.074463 -0.055219 -0.107753 -0.013218  0.060181 -0.017857   \n",
       "\n",
       "         56        57        58        59        60        61        62  \\\n",
       "0 -0.117382 -0.018209  0.045420 -0.249552  0.033651 -0.044680 -0.068578   \n",
       "1 -0.140402  0.083211 -0.089830 -0.174052  0.065776 -0.007650 -0.035333   \n",
       "2 -0.018424 -0.049011 -0.004659 -0.108618 -0.092896  0.052488 -0.055745   \n",
       "3  0.090902 -0.119643  0.101644 -0.278809 -0.023270 -0.145426 -0.127767   \n",
       "4 -0.035505 -0.026005 -0.015050 -0.164568 -0.022893 -0.100307 -0.093009   \n",
       "\n",
       "         63        64        65        66        67        68        69  \\\n",
       "0 -0.040140 -0.078196 -0.103747 -0.085938 -0.124268  0.084961  0.139425   \n",
       "1 -0.020222 -0.063972 -0.043835 -0.057292  0.025688 -0.014404  0.032939   \n",
       "2 -0.026245 -0.115885 -0.088867 -0.083206 -0.038005 -0.043213  0.082052   \n",
       "3  0.014847 -0.141357  0.116096 -0.040685  0.163086  0.051310 -0.020833   \n",
       "4 -0.043648 -0.115108  0.088215 -0.058594  0.083182 -0.031023  0.036656   \n",
       "\n",
       "         70        71        72        73        74        75        76  \\\n",
       "0 -0.037394  0.049886 -0.062134 -0.060832 -0.164551  0.026286 -0.033651   \n",
       "1 -0.044005  0.163601  0.068502  0.020111 -0.079210 -0.042082 -0.054253   \n",
       "2  0.007853  0.028137  0.068420 -0.018188 -0.125366  0.028890 -0.047038   \n",
       "3  0.000651  0.038656  0.109090  0.056274  0.036662 -0.135010  0.173177   \n",
       "4 -0.252093  0.015939  0.057425 -0.040771  0.018973 -0.101493  0.013463   \n",
       "\n",
       "         77        78        79        80        81        82        83  \\\n",
       "0  0.038818 -0.057373  0.061015 -0.099528  0.092122  0.036601 -0.060507   \n",
       "1  0.024417  0.028049 -0.008803 -0.091295  0.044915  0.014499  0.044440   \n",
       "2  0.097249  0.049438  0.085449 -0.035116  0.043783  0.017253 -0.033530   \n",
       "3 -0.001648  0.047770  0.042867 -0.000376  0.179932  0.028971 -0.080485   \n",
       "4  0.112584 -0.076538  0.080444 -0.059518  0.022269 -0.071045  0.009600   \n",
       "\n",
       "         84        85        86        87        88        89        90  \\\n",
       "0  0.082275 -0.030721 -0.009196  0.103404  0.001943  0.038981  0.123454   \n",
       "1  0.064602  0.002992 -0.083442  0.126394  0.094445  0.018419  0.009772   \n",
       "2  0.036051  0.019368 -0.002686  0.045741 -0.099609 -0.028239  0.037282   \n",
       "3  0.221110 -0.100530 -0.083669  0.044678  0.072825  0.065857  0.100016   \n",
       "4  0.101998 -0.048429 -0.178327  0.152344  0.002829  0.005332  0.140695   \n",
       "\n",
       "         91        92        93        94        95        96        97  \\\n",
       "0  0.011963 -0.078763 -0.101461 -0.139242  0.086426 -0.044942 -0.051346   \n",
       "1  0.035136 -0.071571 -0.076823 -0.079915  0.030599  0.074232 -0.063477   \n",
       "2 -0.048869 -0.063853  0.007222  0.030436 -0.004842 -0.002604  0.083659   \n",
       "3 -0.232422 -0.104762 -0.052999 -0.110026 -0.049316 -0.028941 -0.026886   \n",
       "4 -0.120571  0.003701  0.034738 -0.142273  0.101038 -0.035359 -0.086966   \n",
       "\n",
       "         98        99       100       101       102       103       104  \\\n",
       "0  0.214478  0.051982 -0.033285  0.013041  0.002767  0.002828  0.073032   \n",
       "1  0.212023  0.019463 -0.023383 -0.095744  0.061964  0.021362  0.050300   \n",
       "2  0.107605 -0.021207 -0.061615  0.017141  0.034180  0.029683  0.012526   \n",
       "3  0.086670 -0.050975 -0.037699 -0.076853  0.039302 -0.080729 -0.055893   \n",
       "4  0.190186  0.037598  0.054199 -0.043039  0.047259  0.042759  0.041024   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0  0.163574 -0.208537  0.020345  0.135742 -0.006714 -0.044881 -0.061890   \n",
       "1 -0.029961 -0.145989  0.000188  0.200928 -0.016330 -0.053699 -0.075765   \n",
       "2  0.042427 -0.126465  0.021342  0.041280 -0.003894 -0.093953 -0.065308   \n",
       "3 -0.058716 -0.089254  0.033854  0.128062 -0.006266 -0.094218 -0.065043   \n",
       "4 -0.114572 -0.101214 -0.070565  0.161482 -0.078108 -0.028477 -0.098319   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0  0.051839 -0.026937  0.045980  0.052348 -0.023600 -0.276123  0.043335   \n",
       "1 -0.044156  0.034451 -0.007209  0.007704  0.083679 -0.012804  0.059699   \n",
       "2  0.026367  0.046895  0.025065  0.051971 -0.033732 -0.051707  0.129374   \n",
       "3 -0.105754  0.087321  0.066366  0.071757 -0.018575  0.010579  0.084330   \n",
       "4 -0.035052 -0.018206  0.108965  0.065918  0.005214 -0.074986  0.216518   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0  0.084157  0.105428 -0.003113 -0.206991  0.110575  0.118327 -0.052622   \n",
       "1  0.046251 -0.054959  0.059998 -0.216166  0.022366  0.051676 -0.096489   \n",
       "2  0.061035  0.025960 -0.017832 -0.104167 -0.005188 -0.082194 -0.022909   \n",
       "3  0.060598 -0.108067 -0.050863 -0.081533  0.037760  0.151693 -0.134115   \n",
       "4  0.094064 -0.088867  0.030814 -0.114650  0.086496  0.052891  0.030518   \n",
       "\n",
       "        126       127       128       129       130       131       132  \\\n",
       "0 -0.067342  0.039185  0.058289 -0.039581 -0.196574  0.026133  0.011617   \n",
       "1 -0.057265 -0.034846  0.028727 -0.019931 -0.085795  0.005164  0.017819   \n",
       "2 -0.034485  0.019012  0.137858  0.030426 -0.060527 -0.097707  0.058024   \n",
       "3 -0.038045  0.126099 -0.011058  0.009421 -0.182190  0.015696 -0.063599   \n",
       "4 -0.088191  0.081020 -0.030252  0.023577 -0.175293 -0.041794 -0.133440   \n",
       "\n",
       "        133       134       135       136       137       138       139  \\\n",
       "0  0.023051 -0.109202  0.043915 -0.011190 -0.012197  0.048096  0.011353   \n",
       "1 -0.031386 -0.010615 -0.002547 -0.056986  0.045627  0.051439  0.078362   \n",
       "2 -0.041321 -0.021011 -0.081706  0.082845 -0.010966  0.043193  0.052958   \n",
       "3 -0.038005 -0.198873 -0.048197  0.177531  0.087321 -0.027832  0.027425   \n",
       "4 -0.080314 -0.114816  0.110439 -0.003209 -0.002853 -0.061157  0.063041   \n",
       "\n",
       "        140       141       142       143       144       145       146  \\\n",
       "0  0.288900 -0.083293 -0.062876 -0.053426  0.064270 -0.034261 -0.005025   \n",
       "1  0.110548 -0.084496  0.077610  0.091607 -0.032996  0.047377 -0.034858   \n",
       "2  0.082397 -0.082560 -0.038488  0.058573 -0.042765  0.000732 -0.061991   \n",
       "3  0.062907 -0.031504  0.108826 -0.110575 -0.008031 -0.043432 -0.001465   \n",
       "4  0.066333  0.031791 -0.063198  0.000349  0.088797 -0.110897  0.039873   \n",
       "\n",
       "        147       148       149       150       151       152       153  \\\n",
       "0 -0.016693 -0.088582 -0.205322  0.116984 -0.101969 -0.003977 -0.069417   \n",
       "1 -0.051805 -0.079793 -0.083378  0.150011 -0.073107 -0.121585 -0.012478   \n",
       "2 -0.068532  0.015747 -0.024292  0.074849  0.031820 -0.092250 -0.020162   \n",
       "3 -0.092855  0.064677 -0.066732 -0.019257  0.163859 -0.047719 -0.007812   \n",
       "4 -0.124176 -0.042899 -0.087123 -0.062901  0.036656 -0.020194  0.068593   \n",
       "\n",
       "        154       155       156       157       158       159       160  \\\n",
       "0  0.023163 -0.142741 -0.004181 -0.026530  0.065626 -0.038544 -0.201538   \n",
       "1  0.027656 -0.062822 -0.024272 -0.102905 -0.051975 -0.023164 -0.190809   \n",
       "2  0.084524 -0.147868 -0.062541  0.020198 -0.021729 -0.030426 -0.085693   \n",
       "3  0.065715 -0.017843 -0.023214 -0.034251 -0.024292 -0.148397 -0.087591   \n",
       "4  0.078413 -0.077096 -0.024309 -0.079450 -0.037545 -0.133440 -0.023856   \n",
       "\n",
       "        161       162       163       164       165       166       167  \\\n",
       "0  0.103404 -0.099365  0.042969 -0.020793 -0.140340  0.140372 -0.044189   \n",
       "1  0.129924  0.046678 -0.003092 -0.100179 -0.093669  0.057292 -0.047797   \n",
       "2  0.051463 -0.010905  0.030680  0.065308 -0.189209  0.003206 -0.046076   \n",
       "3  0.003743 -0.013021  0.099426 -0.027995 -0.118225  0.046631 -0.015864   \n",
       "4  0.125244 -0.049037  0.053502 -0.112549  0.016881  0.063267 -0.008292   \n",
       "\n",
       "        168       169       170       171       172       173       174  \\\n",
       "0 -0.116414  0.194603 -0.135277  0.025309  0.029427 -0.008776  0.021322   \n",
       "1 -0.021681  0.064168 -0.090251  0.151048 -0.054837  0.012478 -0.058757   \n",
       "2 -0.103394  0.090983 -0.017095  0.004924  0.033162 -0.115377 -0.037476   \n",
       "3 -0.100952  0.055786  0.002808 -0.030111  0.000854 -0.026100  0.104655   \n",
       "4 -0.042524  0.052420  0.039008  0.071472  0.006378 -0.009972  0.055612   \n",
       "\n",
       "        175       176       177       178       179       180       181  \\\n",
       "0 -0.011882  0.162964 -0.080648  0.194173 -0.153809 -0.049357 -0.024295   \n",
       "1  0.054199  0.029473 -0.028904 -0.003152 -0.108188 -0.082987 -0.154588   \n",
       "2  0.015483  0.017873 -0.157227  0.124105 -0.126567 -0.146362 -0.134094   \n",
       "3 -0.000992  0.109594 -0.123479 -0.149984  0.008138 -0.202881 -0.073079   \n",
       "4  0.000035  0.138939 -0.071307 -0.028948  0.031738 -0.279088 -0.097377   \n",
       "\n",
       "        182       183       184       185       186       187       188  \\\n",
       "0 -0.039917 -0.067261  0.164591  0.001709 -0.085144  0.081889  0.068522   \n",
       "1 -0.089762 -0.082418  0.111226  0.024061 -0.054118  0.054050  0.014988   \n",
       "2  0.007182 -0.084229  0.075480 -0.031128 -0.122884  0.020467  0.027629   \n",
       "3  0.032227 -0.105367  0.041260 -0.039968 -0.138021  0.154226  0.035797   \n",
       "4 -0.038574  0.036447 -0.005057  0.015128 -0.153721  0.149937 -0.082419   \n",
       "\n",
       "        189       190       191       192       193       194       195  \\\n",
       "0  0.031911  0.046265  0.115234 -0.217855 -0.105306  0.009359  0.091675   \n",
       "1  0.085395  0.050117  0.028890 -0.066603 -0.016398  0.091539  0.080098   \n",
       "2  0.117645  0.052917  0.036031 -0.062907 -0.020345  0.006104  0.075867   \n",
       "3 -0.044647 -0.149801 -0.012451 -0.048991 -0.069743 -0.082250  0.074097   \n",
       "4  0.012626 -0.013951  0.057329 -0.043588 -0.100307 -0.117083  0.140224   \n",
       "\n",
       "        196       197       198       199       200       201       202  \\\n",
       "0 -0.076823  0.013916 -0.052916 -0.074188 -0.033691  0.140869 -0.030874   \n",
       "1 -0.097620  0.049981 -0.094672 -0.032235  0.027303  0.088460  0.057590   \n",
       "2  0.072021 -0.011475  0.079956 -0.119954  0.015035  0.131185 -0.037038   \n",
       "3  0.025508  0.002035 -0.090495 -0.193359 -0.033590  0.076864  0.041880   \n",
       "4 -0.054478 -0.083705  0.082014 -0.260812 -0.080963  0.151672 -0.012748   \n",
       "\n",
       "        203       204       205       206       207       208       209  \\\n",
       "0 -0.009588  0.016388  0.022868  0.050496  0.015767 -0.088623  0.058350   \n",
       "1 -0.003974  0.018053 -0.060452  0.029351  0.082648 -0.045213 -0.010520   \n",
       "2 -0.102656  0.012527 -0.000020 -0.048421  0.088994 -0.032928 -0.013875   \n",
       "3 -0.001863  0.023560  0.022263  0.033488  0.060735  0.156667 -0.030070   \n",
       "4 -0.011300 -0.022757  0.037545 -0.067191 -0.104614 -0.008057  0.092006   \n",
       "\n",
       "        210       211       212       213       214       215       216  \\\n",
       "0  0.028239  0.061117 -0.135228 -0.064412  0.005178 -0.021576  0.137166   \n",
       "1  0.071452  0.032315 -0.012261 -0.031019 -0.127360  0.060023  0.043121   \n",
       "2  0.068014 -0.049927 -0.080343 -0.006144 -0.068746  0.037667  0.116577   \n",
       "3  0.041056  0.186727 -0.068985 -0.060099 -0.156148  0.000122  0.069865   \n",
       "4 -0.001378  0.156948 -0.057055 -0.050502 -0.141741  0.073382  0.109767   \n",
       "\n",
       "        217       218       219       220       221       222       223  \\\n",
       "0 -0.050954 -0.035294  0.005249 -0.059123  0.061017  0.051758  0.049927   \n",
       "1  0.030952 -0.147651 -0.020718 -0.153849  0.042013  0.033189  0.031711   \n",
       "2 -0.071228 -0.026810  0.022125 -0.042013 -0.005857 -0.082031  0.038574   \n",
       "3  0.035238 -0.099202  0.013509 -0.220357  0.093180 -0.028727 -0.059021   \n",
       "4  0.082275 -0.014391 -0.025940 -0.014805  0.055934 -0.062047  0.025705   \n",
       "\n",
       "        224       225       226       227       228       229       230  \\\n",
       "0 -0.085276 -0.061554 -0.017375  0.044271 -0.014038 -0.086426  0.107076   \n",
       "1  0.058556 -0.107727  0.099962  0.140815  0.003347  0.050496  0.038591   \n",
       "2 -0.032837  0.036977  0.109833 -0.005208  0.034871 -0.023722  0.002177   \n",
       "3 -0.006734 -0.141429  0.065959 -0.033930  0.068970 -0.030887 -0.142741   \n",
       "4 -0.028050 -0.090768  0.048671 -0.003732 -0.025142 -0.002267  0.000419   \n",
       "\n",
       "        231       232       233       234       235       236       237  \\\n",
       "0  0.040548 -0.073354 -0.019613  0.024592 -0.007266  0.072306  0.013509   \n",
       "1 -0.003052 -0.030599  0.005941  0.086928 -0.151598  0.137112 -0.047292   \n",
       "2 -0.007650 -0.021139 -0.093669  0.015549  0.043538  0.039879 -0.027364   \n",
       "3  0.033223 -0.004842 -0.099325  0.047994 -0.082382 -0.091553 -0.073730   \n",
       "4  0.131958  0.014988 -0.085118  0.112462 -0.132882  0.061558 -0.120219   \n",
       "\n",
       "        238       239       240       241       242       243       244  \\\n",
       "0  0.056966  0.037781  0.123942  0.008850  0.008362 -0.000977 -0.041504   \n",
       "1  0.124308 -0.011943  0.043047  0.029912 -0.056634 -0.078559  0.008586   \n",
       "2  0.059896 -0.011678  0.000992 -0.040649 -0.019389  0.065816 -0.001719   \n",
       "3  0.033061  0.075890  0.047760 -0.030111 -0.116648 -0.058116 -0.037130   \n",
       "4  0.115653 -0.071987 -0.088632  0.068566 -0.002302 -0.085798 -0.021868   \n",
       "\n",
       "        245       246       247       248       249       250       251  \\\n",
       "0 -0.034953  0.082886  0.124756 -0.125610 -0.190776 -0.022278  0.061839   \n",
       "1  0.158098  0.031715  0.105360 -0.048394 -0.185371  0.088294  0.030467   \n",
       "2 -0.015747 -0.013428  0.040873 -0.040639 -0.069377  0.081657  0.058044   \n",
       "3 -0.162435 -0.027201  0.134768  0.033610 -0.294027  0.055562  0.036784   \n",
       "4 -0.117746 -0.038783  0.058184 -0.040423 -0.178545 -0.037197  0.107509   \n",
       "\n",
       "        252       253       254       255       256       257       258  \\\n",
       "0  0.049581  0.134206  0.148641 -0.135742  0.031158  0.124959 -0.066650   \n",
       "1 -0.041006  0.192561  0.132670 -0.038194  0.107279  0.047092 -0.034917   \n",
       "2  0.005249  0.117432  0.001017 -0.000264 -0.014323  0.067871 -0.020671   \n",
       "3 -0.072001  0.126592  0.063838 -0.075948 -0.034203 -0.064128 -0.140625   \n",
       "4 -0.017661  0.093157  0.087437 -0.090663  0.080645  0.064453 -0.106759   \n",
       "\n",
       "        259       260       261       262       263       264       265  \\\n",
       "0 -0.042318 -0.003052  0.022003  0.155924 -0.134644  0.012227  0.106415   \n",
       "1 -0.058675  0.056722 -0.017042  0.149902  0.065286  0.030535 -0.014648   \n",
       "2 -0.075439  0.097026 -0.049601  0.070302  0.055786 -0.001801  0.117442   \n",
       "3 -0.131999 -0.014160 -0.016856  0.046305  0.146535 -0.031982 -0.058634   \n",
       "4 -0.033343 -0.038156 -0.006701  0.027335  0.049127 -0.015246  0.193080   \n",
       "\n",
       "        266       267       268       269       270       271       272  \\\n",
       "0 -0.092285 -0.039612  0.009603  0.047201 -0.021126 -0.047668  0.055562   \n",
       "1 -0.105659 -0.004008  0.031223 -0.032511  0.034912  0.082954  0.102146   \n",
       "2 -0.045333 -0.012736  0.039469  0.001149 -0.018555  0.013855  0.096029   \n",
       "3  0.040497 -0.056701 -0.164917 -0.027507  0.091838  0.167450  0.049459   \n",
       "4 -0.068952 -0.006553 -0.152274 -0.011509 -0.079381  0.107561  0.004373   \n",
       "\n",
       "        273       274       275       276       277       278       279  \\\n",
       "0  0.008235  0.174601  0.015889  0.009638 -0.065643 -0.065084  0.138672   \n",
       "1 -0.021088  0.057034  0.029705 -0.089681 -0.048299 -0.004015  0.097629   \n",
       "2  0.020218  0.075846 -0.077474 -0.045939 -0.027669 -0.007100  0.138590   \n",
       "3 -0.087626  0.018893 -0.041168 -0.044678 -0.025584  0.062459  0.109413   \n",
       "4 -0.004211 -0.046247  0.041643  0.037737  0.025225  0.056946 -0.010141   \n",
       "\n",
       "        280       281       282       283       284       285       286  \\\n",
       "0 -0.088623  0.075439 -0.124858  0.071592  0.053894  0.044313 -0.137777   \n",
       "1 -0.077728  0.071031 -0.184252  0.097553  0.007589 -0.013075 -0.156467   \n",
       "2 -0.054138  0.027913 -0.041967  0.012540 -0.050761 -0.002299 -0.036357   \n",
       "3 -0.176514  0.069519  0.017263  0.059255 -0.066406  0.060918 -0.163411   \n",
       "4 -0.023019 -0.053563 -0.034999  0.099792 -0.096793 -0.012102 -0.139858   \n",
       "\n",
       "        287       288       289       290       291       292       293  \\\n",
       "0  0.157715  0.072449  0.021647  0.083008  0.081746  0.038116  0.093424   \n",
       "1  0.078722  0.075358  0.077284  0.047272  0.132711  0.009962  0.031087   \n",
       "2 -0.027110  0.082621  0.015381  0.002625  0.026367  0.099396  0.053792   \n",
       "3  0.153905  0.163147  0.088826  0.003337  0.073853 -0.075114  0.035563   \n",
       "4  0.112514  0.174438  0.099505 -0.081822 -0.012748 -0.051200 -0.071638   \n",
       "\n",
       "        294       295       296       297       298       299  Sentiment  \\\n",
       "0  0.033590 -0.137166 -0.109904 -0.055583 -0.098694 -0.025431          0   \n",
       "1 -0.015971 -0.003174 -0.017293 -0.091109  0.025137 -0.040622          0   \n",
       "2  0.046010 -0.034454  0.017446 -0.030070  0.043457  0.081278          0   \n",
       "3  0.015666  0.000285  0.156169 -0.040385 -0.123149 -0.069814          0   \n",
       "4 -0.083104  0.021170  0.153721 -0.019165 -0.055054 -0.116455          0   \n",
       "\n",
       "   Company_Clarivate  Company_Informa  Company_Pearson  Company_RELX Group  \\\n",
       "0                  0                0                0                   0   \n",
       "1                  0                0                0                   0   \n",
       "2                  0                0                0                   0   \n",
       "3                  0                0                0                   0   \n",
       "4                  0                1                0                   0   \n",
       "\n",
       "   Company_Thomson Reuters  Company_Wolters Kluwer  Country 2_Argentina  \\\n",
       "0                        1                       0                    0   \n",
       "1                        1                       0                    0   \n",
       "2                        1                       0                    0   \n",
       "3                        1                       0                    0   \n",
       "4                        0                       0                    0   \n",
       "\n",
       "   Country 2_Australia  Country 2_Belgium  Country 2_Brazil  Country 2_Canada  \\\n",
       "0                    0                  0                 0                 0   \n",
       "1                    0                  0                 0                 0   \n",
       "2                    0                  0                 0                 0   \n",
       "3                    0                  0                 0                 0   \n",
       "4                    0                  0                 0                 0   \n",
       "\n",
       "   Country 2_Ecuador  Country 2_France  Country 2_Germany  \\\n",
       "0                  0                 0                  0   \n",
       "1                  0                 0                  0   \n",
       "2                  0                 0                  0   \n",
       "3                  0                 0                  0   \n",
       "4                  0                 0                  0   \n",
       "\n",
       "   Country 2_Hong Kong  Country 2_India  Country 2_Italy  Country 2_Japan  \\\n",
       "0                    0                0                0                0   \n",
       "1                    0                0                0                0   \n",
       "2                    0                0                0                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "\n",
       "   Country 2_Mexico  Country 2_Netherlands  Country 2_Other  \\\n",
       "0                 0                      0                0   \n",
       "1                 0                      0                1   \n",
       "2                 0                      0                0   \n",
       "3                 0                      0                0   \n",
       "4                 0                      0                0   \n",
       "\n",
       "   Country 2_Philippines  Country 2_Russia  Country 2_Serbia  \\\n",
       "0                      0                 0                 0   \n",
       "1                      0                 0                 0   \n",
       "2                      0                 0                 0   \n",
       "3                      0                 0                 0   \n",
       "4                      0                 0                 0   \n",
       "\n",
       "   Country 2_Singapore  Country 2_South Africa  Country 2_Spain  \\\n",
       "0                    0                       0                0   \n",
       "1                    0                       0                0   \n",
       "2                    0                       0                0   \n",
       "3                    0                       0                0   \n",
       "4                    0                       0                0   \n",
       "\n",
       "   Country 2_Switzerland  Country 2_United Arab Emirates  \\\n",
       "0                      0                               0   \n",
       "1                      0                               0   \n",
       "2                      0                               0   \n",
       "3                      0                               0   \n",
       "4                      0                               0   \n",
       "\n",
       "   Country 2_United Kingdom  Country 2_United States  Country 2_Venezuela  \\\n",
       "0                         0                        1                    0   \n",
       "1                         0                        0                    0   \n",
       "2                         0                        1                    0   \n",
       "3                         1                        0                    0   \n",
       "4                         0                        1                    0   \n",
       "\n",
       "   ALL_Thread_Entry_Type_post  ALL_Thread_Entry_Type_reply  \\\n",
       "0                           0                            0   \n",
       "1                           0                            0   \n",
       "2                           1                            0   \n",
       "3                           1                            0   \n",
       "4                           0                            0   \n",
       "\n",
       "   ALL_Thread_Entry_Type_share  TW_Account_Type_Not identified  \\\n",
       "0                            1                               0   \n",
       "1                            1                               0   \n",
       "2                            0                               0   \n",
       "3                            0                               0   \n",
       "4                            1                               0   \n",
       "\n",
       "   TW_Account_Type_individual  TW_Account_Type_organisational  ALL_Impact  \\\n",
       "0                           1                               0          28   \n",
       "1                           1                               0          25   \n",
       "2                           0                               1          25   \n",
       "3                           0                               1          64   \n",
       "4                           1                               0          41   \n",
       "\n",
       "   Log_TW_KredOutreach  Log_Nbreach  Log_TW_NbFollowers  Log_TW_NbFollowing  \\\n",
       "0                  0.0     6.052089            4.043051            5.093750   \n",
       "1                  0.0     5.940171            3.295837            4.948760   \n",
       "2                  0.0     5.921578            3.135494            0.000000   \n",
       "3                  0.0     7.832808            6.736967            5.043425   \n",
       "4                  0.0     6.408529            7.094235            7.222566   \n",
       "\n",
       "   Log_TW_NbTweets       TW_Hashtags       ALL_Author  \\\n",
       "0         7.409136      2017btsfesta     titina_joner   \n",
       "1         8.307706     No TW hashtag        dasCameo1   \n",
       "2        12.286371      indeed, jobs  lumpyspace_tst2   \n",
       "3         4.564348   MiFIDII, TRRisk          mifidii   \n",
       "4         8.832150  infographic, iot    daviddefelipe   \n",
       "\n",
       "             TW_Account_Name  All_impact bin  \n",
       "0  tw account not identified             0.0  \n",
       "1  tw account not identified             0.0  \n",
       "2  tw account not identified             0.0  \n",
       "3  tw account not identified             2.0  \n",
       "4            david de felipe             2.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train, validation and test datasets (from the main Train set of data)\n",
    "> * I am facing a lack of computing resources (laptop with i7 Intel chip and 16 Go RAM, no GPU) which implies a very long time for training models, especially with the tuning of hyper-parameters. As a consequence, I have combined my computing resources with Google Colaboratory in order to tune several parameters in parallel.\n",
    "* **The overall dataset is divided in 3 buckets:**\n",
    "* Bucket 1 (train/test): split for training the Best Selected model (in case of more important computing resources)\n",
    "* Bucket 2 (train1/valid1): split for training the Best model candidate of a given class (no cross-validation)\n",
    "* Bucket 3 (train2/valid2): split for hyper-parameter tuning leading to select the Best model candidate (cross-validation maybe considered in some cases)\n",
    "* We could limit the risk of overfitting by using a cross-validation approach. However, we may run the risk of very demanding computing resources as we will combine hyper-parameter optimization (GridSearch) and large dataset (194484 rows x 344 variables).\n",
    "* A compromised approach would be to use the standard train/test dataset split and leverage cross-validation for the validation phase in the process for selecting the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 344)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an array from df_modeling2 excluding the target variable All impact bin\n",
    "X = df_modeling2.drop([\"All_impact bin\"], axis=1)\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create y array for the target variable All impact bin\n",
    "y = df_modeling2[\"All_impact bin\"]\n",
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (155587, 344) (155587,)\n",
      "Test: (38897, 344) (38897,)\n",
      "Train1: (15000, 344) (15000,)\n",
      "Valid1: (4000, 344) (4000,)\n",
      "Train2: (5000, 344) (5000,)\n",
      "Valid2: (1500, 344) (1500,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the type of the input matrix to float\n",
    "X = X.astype(np.float)\n",
    "\n",
    "# Create train set\n",
    "X_tr_main, X_test, y_tr_main, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Create validation and test sets for best model selected for a given class\n",
    "X_tr_2nd, X_valid1, y_tr_2nd, y_valid1 = train_test_split(\n",
    "    X_tr_main, y_tr_main, test_size=4000, train_size=15000, random_state=0\n",
    ")\n",
    "\n",
    "# Create validation and test sets for hyper-parameter tuning and selection of the best model candidate\n",
    "X_tr_3rd, X_valid2, y_tr_3rd, y_valid2 = train_test_split(\n",
    "    X_tr_2nd, y_tr_2nd, test_size=1500, train_size=5000, random_state=0\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_tr_main.shape, y_tr_main.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n",
    "print(\"Train1:\", X_tr_2nd.shape, y_tr_2nd.shape)\n",
    "print(\"Valid1:\", X_valid1.shape, y_valid1.shape)\n",
    "print(\"Train2:\", X_tr_3rd.shape, y_tr_3rd.shape)\n",
    "print(\"Valid2:\", X_valid2.shape, y_valid2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.364667\n",
       "0    0.324000\n",
       "1    0.311333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y_valid2, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Random Forest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest pipeline\n",
    "pipe_rf = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\n",
    "            \"PCA\",\n",
    "            PCA(n_components=200),\n",
    "        ),  # 200 components to explain 95% of the variance (see first part of this notebook)\n",
    "        (\"rf\", RandomForestClassifier(n_jobs=-1, random_state=0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "# pipe_rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the grid of parameters\n",
    "> * I have tested 2 sets of hyper-parameters (6 parameters, version underneath) and 4 hyperparameters (indicated in the section where the confusion matrix is performed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 8\n"
     ]
    }
   ],
   "source": [
    "# Grid of parameters\n",
    "grid_rf = ParameterGrid(\n",
    "    {\n",
    "        \"rf__n_estimators\": [2000],  # of decision trees\n",
    "        \"rf__criterion\": [\"gini\"],  # quality of split\n",
    "        \"rf__max_features\": [\n",
    "            0.5,\n",
    "            0.8,\n",
    "        ],  # % of features vs total number of features for looking at the best split\n",
    "        \"rf__max_depth\": [5, 10],  # depth of trees\n",
    "        \"rf__min_samples_leaf\": [\n",
    "            2,\n",
    "            4,\n",
    "        ],  # # Minimum number of samples required at each leaf node\n",
    "        \"rf__bootstrap\": [\n",
    "            False\n",
    "        ],  # Method of selecting samples for training each tree ('True' tested but do not improve the performance)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the number of combinations\n",
    "print(\"Number of combinations:\", len(grid_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on on sub-train data set (5 000 tweets) and test accuracy on the validation data set (1 500 tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1/8\n",
      "Combination 2/8\n",
      "Combination 3/8\n",
      "Combination 4/8\n",
      "Combination 5/8\n",
      "Combination 6/8\n",
      "Combination 7/8\n",
      "Combination 8/8\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#  Save accuracy on train and validation sets\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "\n",
    "# Enumerate combinations starting from 1\n",
    "for i, params_dict in enumerate(grid_rf, 1):\n",
    "    # Print progress\n",
    "    print(\"Combination {}/{}\".format(i, len(grid_rf)))  # Total number of combinations\n",
    "\n",
    "    # Set parameters\n",
    "    pipe_rf.set_params(**params_dict)\n",
    "\n",
    "    # Fit a Decision Tree classifier\n",
    "    pipe_rf.fit(X_tr_3rd, y_tr_3rd)\n",
    "\n",
    "    # Save accuracy on validation set\n",
    "    params_dict[\"accuracy_train\"] = pipe_rf.score(X_tr_3rd, y_tr_3rd)\n",
    "    params_dict[\"accuracy_valid\"] = pipe_rf.score(X_valid2, y_valid2)\n",
    "\n",
    "    # Save result\n",
    "    train_scores.append(params_dict)\n",
    "    valid_scores.append(params_dict)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf__bootstrap</th>\n",
       "      <th>rf__criterion</th>\n",
       "      <th>rf__max_depth</th>\n",
       "      <th>rf__max_features</th>\n",
       "      <th>rf__min_samples_leaf</th>\n",
       "      <th>rf__n_estimators</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.732667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.731333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7336</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7278</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rf__bootstrap rf__criterion  rf__max_depth  rf__max_features  \\\n",
       "5          False          gini             10               0.5   \n",
       "4          False          gini             10               0.5   \n",
       "0          False          gini              5               0.5   \n",
       "1          False          gini              5               0.5   \n",
       "6          False          gini             10               0.8   \n",
       "7          False          gini             10               0.8   \n",
       "2          False          gini              5               0.8   \n",
       "3          False          gini              5               0.8   \n",
       "\n",
       "   rf__min_samples_leaf  rf__n_estimators  accuracy_train  accuracy_valid  \n",
       "5                     4              2000          0.9692        0.732667  \n",
       "4                     2              2000          0.9812        0.731333  \n",
       "0                     2              2000          0.7348        0.716667  \n",
       "1                     4              2000          0.7336        0.716000  \n",
       "6                     2              2000          0.9676        0.708000  \n",
       "7                     4              2000          0.9612        0.708000  \n",
       "2                     2              2000          0.7278        0.706000  \n",
       "3                     4              2000          0.7268        0.706000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with test scores\n",
    "scores_df = pd.DataFrame(valid_scores)\n",
    "# Print scores\n",
    "scores_df.sort_values(by=\"accuracy_valid\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side notes\n",
    "> * I have tested several combination of hyper-parameters separatly due to the limitation of computing resources (with different size of train/valid datasets, from 5000 to 15000 for train dataset)\n",
    "* It appears that the accuracy rates do not vary much and there is in any case some overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train\taccuracy_valid\trf__bootstrap\trf__criterion\trf__max_depth\trf__max_features\trf__min_samples_leaf\trf__n_estimators\n",
    "# 2\t0.9810\t0.672000\tTrue\tgini\t10\t0.8\t2\t500\n",
    "# 0\t0.9826\t0.669333\tTrue\tgini\t10\t0.5\t2\t500\n",
    "# 1\t0.9716\t0.669333\tTrue\tgini\t10\t0.5\t4\t500\n",
    "# 3\t0.9718\t0.658000\tTrue\tgini\t10\t0.8\t4\t500\n",
    "# 4\t0.9890\t0.656000\tFalse\tgini\t10\t0.5\t2\t500\n",
    "# 5\t0.9862\t0.656000\tFalse\tgini\t10\t0.5\t4\t500\n",
    "# 6\t0.9768\t0.624667\tFalse\tgini\t10\t0.8\t2\t500\n",
    "# 7\t0.9704\t0.622000\tFalse\tgini\t10\t0.8\t4\t500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train\taccuracy_valid\trf__bootstrap\trf__criterion\trf__max_depth\trf__max_features\trf__min_samples_leaf\trf__n_estimators\n",
    "# 2\t0.9810\t0.672000\tTrue\tgini\t10\t0.8\t2\t500\n",
    "# 0\t0.9826\t0.669333\tTrue\tgini\t10\t0.5\t2\t500\n",
    "# 1\t0.9716\t0.669333\tTrue\tgini\t10\t0.5\t4\t500\n",
    "# 3\t0.9718\t0.658000\tTrue\tgini\t10\t0.8\t4\t500\n",
    "# 4\t0.9890\t0.656000\tFalse\tgini\t10\t0.5\t2\t500\n",
    "# 5\t0.9862\t0.656000\tFalse\tgini\t10\t0.5\t4\t500\n",
    "# 6\t0.9768\t0.624667\tFalse\tgini\t10\t0.8\t2\t500\n",
    "# 7\t0.9704\t0.622000\tFalse\tgini\t10\t0.8\t4\t500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train\taccuracy_valid\trf__criterion\trf__max_depth\trf__max_features\trf__n_estimators\n",
    "# 1\t0.9672\t0.698000\tgini\t10\t0.5\t2000\n",
    "# 3\t0.9614\t0.695333\tgini\t10\t0.8\t2000\n",
    "# 2\t0.9622\t0.693333\tgini\t10\t0.8\t500\n",
    "# 0\t0.9668\t0.692000\tgini\t10\t0.5\t500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation confusion matrix on Top 2 models (based on accuracy) with depth of 10 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: (4 hyper-parameters tuned: n_estimators = 2000, criterion = gini , max_features = 0.5, max_depth = 5), depth of 5 to limit overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest pipeline\n",
    "pipe_rf1 = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\n",
    "            \"PCA\",\n",
    "            PCA(n_components=200),\n",
    "        ),  # 200 components to explain 95% of the variance (see first part of this notebook)\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_jobs=-1,\n",
    "                n_estimators=2000,\n",
    "                criterion=\"gini\",\n",
    "                max_features=0.5,\n",
    "                warm_start=True,\n",
    "                max_depth=5,\n",
    "                random_state=0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Decision Tree classifier\n",
    "model_rf1 = pipe_rf1.fit(X_tr_3rd, y_tr_3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on X_valid dataset\n",
    "y_pred_rf1 = pipe_rf1.predict(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      0.81      0.80       486\n",
      "     class 1       0.57      0.48      0.52       467\n",
      "     class 2       0.73      0.80      0.76       547\n",
      "\n",
      "    accuracy                           0.70      1500\n",
      "   macro avg       0.69      0.70      0.69      1500\n",
      "weighted avg       0.70      0.70      0.70      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusions report\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "print(classification_report(y_valid2, y_pred_rf1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 based on larger set of data (train 15 000, test 4 000) - (4 hyper-parameters tuned: n_estimators = 2000, criterion = gini, max_features = 0.5, max_depth = 5), depth of 5 to limit overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest pipeline\n",
    "pipe_rf1b = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\n",
    "            \"PCA\",\n",
    "            PCA(n_components=200),\n",
    "        ),  # 200 components to explain 95% of the variance (see first part of this notebook)\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_jobs=-1,\n",
    "                n_estimators=2000,\n",
    "                criterion=\"gini\",\n",
    "                max_features=0.5,\n",
    "                warm_start=True,\n",
    "                max_depth=5,\n",
    "                random_state=0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Decision Tree classifier\n",
    "model_rf1b = pipe_rf1b.fit(X_tr_2nd, y_tr_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf1b = pipe_rf1b.predict(X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.77      0.79      0.78      1240\n",
      "     class 1       0.59      0.52      0.55      1328\n",
      "     class 2       0.74      0.80      0.77      1432\n",
      "\n",
      "    accuracy                           0.70      4000\n",
      "   macro avg       0.70      0.70      0.70      4000\n",
      "weighted avg       0.70      0.70      0.70      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusions report\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "print(classification_report(y_valid1, y_pred_rf1b, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 (6 hyper-parameters tuned: n_estimators = 2000, min_samples_leaf = 4, bootstrap = False, criterion = gini, max_features = 0.5, max_depth = 10), model with the highest accuracy (but with some overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest pipeline\n",
    "pipe_rf2 = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\n",
    "            \"PCA\",\n",
    "            PCA(n_components=200),\n",
    "        ),  # 200 components to explain 95% of the variance (see first part of this notebook)\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_jobs=-1,\n",
    "                n_estimators=2000,\n",
    "                criterion=\"gini\",\n",
    "                min_samples_leaf=4,\n",
    "                bootstrap=False,\n",
    "                max_features=0.5,\n",
    "                warm_start=True,\n",
    "                max_depth=10,\n",
    "                random_state=0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Decision Tree classifier\n",
    "model_rf2 = pipe_rf2.fit(X_tr_3rd, y_tr_3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on X_valid dataset\n",
    "y_pred_rf2 = pipe_rf2.predict(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.83      0.78      0.81       486\n",
      "     class 1       0.60      0.61      0.61       467\n",
      "     class 2       0.77      0.81      0.79       547\n",
      "\n",
      "    accuracy                           0.74      1500\n",
      "   macro avg       0.74      0.73      0.73      1500\n",
      "weighted avg       0.74      0.74      0.74      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusions report\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "print(classification_report(y_valid2, y_pred_rf2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "> * The Best model candidate for Random Forest classifier is the **model 4** Random Forest, n_estimators: 2 000, max_features = 0.5, max_depth = 5\n",
    "* This model presents acceptable levels of accuracy (0.99 on train and 0.71 on valid). However, the difference between the 2 levels of accuracy indicates important overfitting\n",
    "* We will perform a cross-validation with 5 folds in order to evaluate if we could limit the overfitting despite the small number of datapoints (due to limited computer resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run selected random Forest model on Train/Test dataset using cross-validation with 5 kfolds\n",
    "> * It appears that (probably due to the small number of training dataset), models tends to overfit and do not generalize well. We will try a cross-validation approach with 5 folds in order to see if we could minimize the overfit despite the small number of datapoints (model 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cross validation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest pipeline\n",
    "pipe_rf_grid = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\"PCA\", PCA()),\n",
    "        (\"rf\", RandomForestClassifier(n_jobs=-1, random_state=0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf_grid,\n",
    "    [\n",
    "        {\n",
    "            \"PCA__n_components\": [\n",
    "                200\n",
    "            ],  # nb of components explaining 95% of the variance\n",
    "            \"rf__n_estimators\": [2000],  # of decision trees\n",
    "            \"rf__criterion\": [\"gini\"],  # quality of split\n",
    "            \"rf__max_features\": [\n",
    "                0.5\n",
    "            ],  # % of features vs total number of features for looking at the best split\n",
    "            \"rf__max_depth\": [10],  # depth of trees\n",
    "        }\n",
    "    ],\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('PCA',\n",
       "                                        PCA(copy=True, iterated_power='auto',\n",
       "                                            n_components=None,\n",
       "                                            random_state=None,\n",
       "                                            svd_solver='auto', tol=0.0,\n",
       "                                            whiten=False)),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(bootstrap=True,\n",
       "                                                               class_weight=None,\n",
       "                                                               criterion='gin...\n",
       "                                                               n_estimators='warn',\n",
       "                                                               n_jobs=-1,\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=0,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'PCA__n_components': [200], 'rf__criterion': ['gini'],\n",
       "                          'rf__max_depth': [10], 'rf__max_features': [0.5],\n",
       "                          'rf__n_estimators': [2000]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit estimator\n",
    "grid_rf.fit(X_tr_3rd, y_tr_3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_PCA__n_components', 'param_rf__criterion', 'param_rf__max_depth', 'param_rf__max_features', 'param_rf__n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the results with \"cv_results_\"\n",
    "grid_rf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to collect into the dataframe as arrays\n",
    "pca =  grid_rf.cv_results_[\"param_PCA__n_components\"]\n",
    "n_estimators = grid_rf.cv_results_[\"param_rf__n_estimators\"]\n",
    "criterion = grid_rf.cv_results_[\"param_rf__criterion\"]\n",
    "mean_te = grid_rf.cv_results_[\"mean_test_score\"]\n",
    "std_test_score =  grid_rf.cv_results_[\"std_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th>mean_te</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>2000</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>0.012698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pca n_estimators criterion  mean_te  std_test_score\n",
       "0  200         2000      gini   0.7012        0.012698"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe using from_dict (as from_items is deprecated) and OrderedDict to keep the order\n",
    "df_rf_grid = pd.DataFrame.from_dict(OrderedDict(zip(['pca', 'n_estimators', 'criterion', 'mean_te', 'std_test_score'], [pca, n_estimators, criterion, mean_te, std_test_score])))\n",
    "df_rf_grid.sort_values(by='mean_te', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on Random Forest with cross-validation\n",
    "> * It is not really conclusive as the 2nd best model (with the same set of hyper-parameter tuning) is performing less (accuracy on test 0.8262) than the same one with train/valid dataset (acc. 0.85)\n",
    "* As a consequence, we will select the standard approach which provides slightly better results and being less computational resource demanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for later visualization and overall selection - Best Random Forest Model with 4 hyper-parameters ( n_estimators = 2000, criterion = 'gini', max_features = 0.5, max_depth = 5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc = 0.71\n",
    "c1_rf_f1 = 0.80\n",
    "c2_rf_f1 = 0.53\n",
    "c3_rf_f1 = 0.76\n",
    "\n",
    "%store rf_acc\n",
    "%store c1_rf_f1\n",
    "%store c2_rf_f1\n",
    "%store c3_rf_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
