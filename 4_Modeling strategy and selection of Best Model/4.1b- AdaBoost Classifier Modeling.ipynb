{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4.1b: Modeling AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the methodology\n",
    "> * Finalize bining of Target Variable\n",
    "* Create Train and Test datasets\n",
    "* Create a AdaBoost pipeline\n",
    "* Define key parameters\n",
    "* Run the model on on sub-train data set and test accuracy on the validation data set\n",
    "* Select most accurate model based on the hyper-parameters, run it to get the confusion matrix\n",
    "* Select Best AB model candidate and apply it on a larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "import scikitplot as skplt\n",
    "\n",
    "# t_NSE dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# Activate Seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the file for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the file and creating a dataframe\n",
    "master_modeling = pd.read_csv(\n",
    "    \"master_modeling.csv\",\n",
    "    low_memory=False,\n",
    "    skipinitialspace=True,\n",
    ")  # , sep='\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 351)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the Unnamed column\n",
    "master_modeling.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "master_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the modeling phase (without text and not relevant features)\n",
    "df_modeling = master_modeling.drop([\"Title\", \"Post_ID\", \"Snippet\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 348)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of # of classes for the Target Variable 'All_Impact'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We will split the variable in 3 classes using Scikit Learn preprocessing function KBinDiscretizer with the following parameters: number of bins 3, encode: ordinal and strategy: quantile\n",
    "* Oridinal has been selected as we are trying to model a hierarchy between low and high tweet impact\n",
    "* Quantile implies an even number of data points per class which would shape the model to learn about features for each class equally (avoiding unbalance classes)\n",
    "* We may reconsider some of the value of the parameters depending on the modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_bin = master_modeling[[\"ALL_Impact\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process binizer\n",
    "est = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"quantile\")\n",
    "est.fit(ai_bin)\n",
    "new_ai = est.transform(ai_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., 30., 41., 80.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the edge of the different 3 bins\n",
    "est.bin_edges_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ai_df = pd.DataFrame(new_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ai_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling[\"All_impact bin\"] = new_ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    69658\n",
       "1.0    63049\n",
       "0.0    61777\n",
       "Name: All_impact bin, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling[\"All_impact bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original All_Impact feature\n",
    "df_modeling2 = df_modeling.drop(\n",
    "    [\"ALL_Impact\", \"TW_Hashtags\", \"ALL_Author\", \"TW_Account_Name\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new All Impact feature type into int64\n",
    "df_modeling2[\"All_impact bin\"] = df_modeling2[\"All_impact bin\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 345)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train, validation and test datasets (from the main Train set of data)\n",
    "> * I am facing a lack of computing resources (laptop with i7 Intel chip and 16 Go RAM, no GPU) which implies a very long time for training models, especially with the tuning of hyper-parameters. As a consequence, I have combined my computing resources with Google Colaboratory in order to tune several parameters in parallel.\n",
    "* **The overall dataset is divided in 3 buckets:**\n",
    "* Bucket 1 (train/test): split for training the Best Selected model (in case of more important computing resources)\n",
    "* Bucket 2 (train1/valid1): split for training the Best model candidate of a given class (no cross-validation)\n",
    "* Bucket 3 (train2/valid2): split for hyper-parameter tuning leading to select the Best model candidate (cross-validation maybe considered in some cases)\n",
    "* We could limit the risk of overfitting by using a cross-validation approach. However, we may run the risk of very demanding computing resources as we will combine hyper-parameter optimization (GridSearch) and large dataset (194484 rows x 344 variables).\n",
    "* A compromised approach would be to use the standard train/test dataset split and leverage cross-validation for the validation phase in the process for selecting the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484, 344)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an array from df_modeling2 excluding the target variable All impact bin\n",
    "X = df_modeling2.drop([\"All_impact bin\"], axis=1)\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194484,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create y array for the target variable All impact bin\n",
    "y = df_modeling2[\"All_impact bin\"]\n",
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (155587, 344) (155587,)\n",
      "Test: (38897, 344) (38897,)\n",
      "Train1: (15000, 344) (15000,)\n",
      "Valid1: (4000, 344) (4000,)\n",
      "Train2: (5000, 344) (5000,)\n",
      "Valid2: (1500, 344) (1500,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the type of the input matrix to float\n",
    "X = X.astype(np.float)\n",
    "\n",
    "# Create train set\n",
    "X_tr_main, X_test, y_tr_main, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Create validation and test sets for best model selected for a given class\n",
    "X_tr_2nd, X_valid1, y_tr_2nd, y_valid1 = train_test_split(\n",
    "    X_tr_main, y_tr_main, test_size=4000, train_size=15000, random_state=0\n",
    ")\n",
    "\n",
    "# Create validation and test sets for hyper-parameter tuning and selection of the best model candidate\n",
    "X_tr_3rd, X_valid2, y_tr_3rd, y_valid2 = train_test_split(\n",
    "    X_tr_2nd, y_tr_2nd, test_size=1500, train_size=5000, random_state=0\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_tr_main.shape, y_tr_main.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n",
    "print(\"Train1:\", X_tr_2nd.shape, y_tr_2nd.shape)\n",
    "print(\"Valid1:\", X_valid1.shape, y_valid1.shape)\n",
    "print(\"Train2:\", X_tr_3rd.shape, y_tr_3rd.shape)\n",
    "print(\"Valid2:\", X_valid2.shape, y_valid2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.364667\n",
       "0    0.324000\n",
       "1    0.311333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y_valid2, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a AdaBoost pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe_ab1 = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\"PCA\", PCA(n_components=200)),  # 200 components to explain 95% of the variance\n",
    "        (\"ab\", AdaBoostClassifier(random_state=0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "# pipe_ab1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the grid of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 6\n"
     ]
    }
   ],
   "source": [
    "# Grid of parameters\n",
    "grid_ab1 = ParameterGrid(\n",
    "    {\n",
    "        \"ab__learning_rate\": [0.2, 0.1, 0.01],  # learning rate\n",
    "        \"ab__n_estimators\": [200, 1000],  # of boosting stages\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the number of combinations\n",
    "print(\"Number of combinations:\", len(grid_ab1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on on sub-train data set (5 000 tweets) and test accuracy on the validation data set (1 500 tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1/6\n",
      "Combination 2/6\n",
      "Combination 3/6\n",
      "Combination 4/6\n",
      "Combination 5/6\n",
      "Combination 6/6\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#  Save accuracy on train and validation sets\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "\n",
    "# Enumerate combinations starting from 1\n",
    "for i, params_dict in enumerate(grid_ab1, 1):\n",
    "    # Print progress\n",
    "    print(\"Combination {}/{}\".format(i, len(grid_ab1)))  # Total number of combinations\n",
    "\n",
    "    # Set parameters\n",
    "    pipe_ab1.set_params(**params_dict)\n",
    "\n",
    "    # Fit a Decision Tree classifier\n",
    "    pipe_ab1.fit(X_tr_3rd, y_tr_3rd)\n",
    "\n",
    "    # Save accuracy on validation set\n",
    "    params_dict[\"accuracy_train\"] = pipe_ab1.score(X_tr_3rd, y_tr_3rd)\n",
    "    params_dict[\"accuracy_valid\"] = pipe_ab1.score(X_valid2, y_valid2)\n",
    "\n",
    "    # Save result\n",
    "    train_scores.append(params_dict)\n",
    "    valid_scores.append(params_dict)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab__learning_rate</th>\n",
       "      <th>ab__n_estimators</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.740667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7346</td>\n",
       "      <td>0.731333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.719333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6956</td>\n",
       "      <td>0.701333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.646667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab__learning_rate  ab__n_estimators  accuracy_train  accuracy_valid\n",
       "1               0.20              1000          0.7818        0.740667\n",
       "3               0.10              1000          0.7606        0.734000\n",
       "0               0.20               200          0.7346        0.731333\n",
       "2               0.10               200          0.7196        0.719333\n",
       "5               0.01              1000          0.6956        0.701333\n",
       "4               0.01               200          0.6364        0.646667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with test scores\n",
    "scores_df = pd.DataFrame(valid_scores)\n",
    "# Print scores\n",
    "scores_df.sort_values(by=\"accuracy_valid\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on AdaBoost classifer\n",
    "> * Best AdaBoost delivers an accuracy on valid dataset (0.7413 vs 0.7854 on Train) whch is slightly better than the Best Random Forest model (0.71 vs 0.99 on Valid)\n",
    "* However, AdaBoost overfits much less than Random Forest. We have to highlight that the number of trees is not the same between the 2 classes of classifier (2000 for the Random forest model and 1000 for the AdaBoost) which can explain part of the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation confusion matrix of most accurate model (based on accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st model (2 hyper-parameters tuned: learning rate, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe_ab2 = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\"PCA\", PCA(n_components=200)),  # 200 components to explain 95% of the variance\n",
    "        (\n",
    "            \"ab\",\n",
    "            AdaBoostClassifier(learning_rate=0.2, n_estimators=1000, random_state=0),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Decision Tree classifier\n",
    "model_ab2 = pipe_ab2.fit(X_tr_3rd, y_tr_3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on X_valid dataset\n",
    "y_pred_ab2 = pipe_ab2.predict(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.73      0.82       486\n",
      "     class 1       0.56      0.84      0.67       467\n",
      "     class 2       0.89      0.68      0.77       547\n",
      "\n",
      "    accuracy                           0.75      1500\n",
      "   macro avg       0.79      0.75      0.75      1500\n",
      "weighted avg       0.80      0.75      0.75      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusions report\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "print(classification_report(y_valid2, y_pred_ab2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st model based on larger set of data (train 15 000, test 4 000) - (2 hyper-parameters tuned: learning rate, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe_ab3 = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # with standardization StandardScaler()\n",
    "        (\"PCA\", PCA(n_components=200)),  # 200 components to explain 95% of the variance\n",
    "        (\n",
    "            \"ab\",\n",
    "            AdaBoostClassifier(learning_rate=0.2, n_estimators=1000, random_state=0),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Decision Tree classifier\n",
    "model_ab3 = pipe_ab3.fit(X_tr_2nd, y_tr_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on X_valid dataset\n",
    "y_pred_ab3 = pipe_ab3.predict(X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.77      0.83      1240\n",
      "     class 1       0.62      0.82      0.71      1328\n",
      "     class 2       0.87      0.72      0.79      1432\n",
      "\n",
      "    accuracy                           0.77      4000\n",
      "   macro avg       0.80      0.77      0.78      4000\n",
      "weighted avg       0.80      0.77      0.78      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusions report\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "print(classification_report(y_valid1, y_pred_ab3, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "> * Performances on a larger dateset does not bring a huge improvement in terms of performances (both accuracy and f1 score)\n",
    "* It may be due to the fact that either themodel captured the essence of the data structure even with a smaller dataste (5 000 vs 15 000) and/or that it would require to apply the model to the entire dataset (194 500 tweets) to get a picture closer to the reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for later visualization and overall selection - Best AdaBoost Model with 2 hyper-parameters (learning_rate = 0.2, n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost_acc = 0.75\n",
    "c1_ab_f1 = 0.82\n",
    "c2_ab_f1 = 0.67\n",
    "c3_ab_f1 = 0.77\n",
    "\n",
    "%store AdaBoost_acc\n",
    "%store c1_ab_f1\n",
    "%store c2_ab_f1\n",
    "%store c3_ab_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
