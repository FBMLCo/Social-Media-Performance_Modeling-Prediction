{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Project Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall pipeline\n",
    "\n",
    "#### **Reference dataset csv files**\n",
    "> * These files are saved on the Google Drive:\n",
    "https://drive.google.com/open?id=1UbZYS3a3uVje_v7U09g3feXzZgDjJjoD\n",
    "* **Step 1**: Datasets used for the EDA: **Twitter_EDA2_train.csv, Master_test_final**\n",
    "* **Step 2**:Datasets of processed/engineering numercial & categorical features:: **materdata_ml.csv**\n",
    "* **Step 3**:Datasets coming out the NLP analysis: **master_modeling.csv**\n",
    "* **Step 5**:Datasets used for the final evaluation: **smv_train_60k.csv, final_modeling.csv**\n",
    "\n",
    "#### **STEP 1: Preliminary data cleaning in order to define the final subset of data for the project**\n",
    "> * Please check the Notebook: **1- Preliminary data cleaning Final**\n",
    "* The subset of data proposed for the project is focused on posts issued via Twitter only.\n",
    "* Check data type and adapt format\n",
    "* Preliminary feature selection: correlated features, missing values\n",
    "* Address NaN and zero values\n",
    "* Cleaning text feature data\n",
    "* Splitting data into Train and Test datasets\n",
    "* Saving files in CSV format\n",
    "\n",
    "\n",
    "#### **STEP 2: Exploratory Data Analysis: Numerical & categorical features (Tweeter_EDA)**\n",
    "> * Please check the Notebook: **2- Twitter_EDA_Final**\n",
    "* Saved csv files resulting from this step: 'masterdata_ml.csv'\n",
    "* Number of tweets per company\n",
    "* Sentiment analysis (Total and split per company)\n",
    "* Origin of posts - Top 15 countries in terms of number of tweets and split per company\n",
    "* Analysis of tweet characteristics\n",
    "* Visualization of tweets over time and identification of outliers per company\n",
    "* Analysis of numerical features and feature engineering\n",
    "* Analysis of categorical features and feature engineering\n",
    "* Conclusions\n",
    "\n",
    "#### **STEP 3: NLP Analysis**\n",
    "> * Please check the Notebook: **3- NLP_Analysis_Final**\n",
    "* Saved csv files resulting from this step:\n",
    "* Analyze Snippet and Hashtags features with Wordcloud\n",
    "* Pre-processing of \"Snippet\" feature\n",
    "* Evaluation of text transformation and word embeding strategy\n",
    "* Perform word embeding and visualize the it using t-SNE dimensionality reduction and conclusions\n",
    "* Perform snippet embeding via averaging word embeding for each of them\n",
    "* Visualize snippet embeding using t-SNE dimensionality reduction and conclusions\n",
    "* Evaluate the optimal number of components for PCA dimension reduction\n",
    "* Consolidate text embedding into a dataframe with numeric features engineering during the EDA + reduce/not reduced dimension NLP features\n",
    "\n",
    "\n",
    "#### **STEP 4: Modeling strategy and selection of Best Model**\n",
    "> * Please check the Notebook: **4- Modeling strategy and selection of Best Model, 4.1 Random Forest Modeling, 4.1b AdaBoot Classifier Modeling, 4.2 SVM_Modeling, 4.3 Summary of Modeling Results**\n",
    "* Saved csv files resulting from this step:\n",
    "* Finalisation of the class definition of the Target Variable (ALL_Impact) and company related feature\n",
    "* Model selection performance indicators (e.g. Accuracy, confusion matrix and associated metrics: precision, recall and F1 score)\n",
    "* Split of dataset (train & test/validate samples)\n",
    "* Definition of the Baseline model: Most frequent class would be considered (naive Bayes and Logistic regression could be selected as alternatives)\n",
    "* Results of Baseline model selected\n",
    "* Description of the approach for minimizing overfitting: cross-validation with stratified k-folds\n",
    "* Tuning of PCA for dimension reduction\n",
    "* Rationale of the 2 types of model classifier selected: SVM and Random Forest\n",
    "* Creation of a pipeline, hyperparameter tuning via GridSearch\n",
    "* Classifier model training (train & test/validate sample)\n",
    "* Model evaluation for each type of classifier vs Baseline (depending on hyperparameter tuning results) and selection of Best Model\n",
    "\n",
    "#### **STEP 5: Application of Best Model on Test dataset**\n",
    "> * Please check the Notebook: **5a Best Model on Master Data,5b. Final Model and Predictions**\n",
    "* Saved csv files resulting from this step:\n",
    "* Processing of the Test dataset based on EDA and requirements from Best Model\n",
    "* Remove actual Target Variable from the Test dataset\n",
    "* Application of Best Model to the master Test dataset\n",
    "* Comparison of results vs actual Target Variable\n",
    "* Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
